{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":9129.643687,"end_time":"2023-08-23T19:48:20.542893","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-08-23T17:16:10.899206","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport argparse\nimport math\nimport os\nimport matplotlib.pyplot as plt\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torch.utils.data as data\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision\nfrom __future__ import print_function\nimport torch.optim as optim\nfrom sklearn.metrics import f1_score\nimport torch.backends.cudnn as cudnn\nfrom torchvision import transforms, datasets\nimport sys\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":75.616131,"end_time":"2023-08-23T17:17:39.763149","exception":false,"start_time":"2023-08-23T17:16:24.147018","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:31.416251Z","iopub.execute_input":"2023-09-03T06:06:31.417239Z","iopub.status.idle":"2023-09-03T06:06:35.542686Z","shell.execute_reply.started":"2023-09-03T06:06:31.417207Z","shell.execute_reply":"2023-09-03T06:06:35.541702Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"#used\ndef parse_option(args):\n    parser = argparse.ArgumentParser('argument for training')\n\n    parser.add_argument('--print_freq', type=int, default=10,\n                        help='print frequency')\n    parser.add_argument('--save_freq', type=int, default=50,\n                        help='save frequency')\n    parser.add_argument('--batch_size', type=int, default=128,\n                        help='batch_size')\n    parser.add_argument('--num_workers', type=int, default=2,\n                        help='num of workers to use')\n    parser.add_argument('--epochs', type=int, default=100,\n                        help='number of training epochs')\n    parser.add_argument('--device', type=str, default='cuda:0')\n    # optimization\n    parser.add_argument('--learning_rate', type=float, default=0.05,\n                        help='learning rate')\n    parser.add_argument('--patient_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--cluster_lambda', type=float, default=1,\n                        help='learning rate')\n    parser.add_argument('--lr_decay_epochs', type=str, default='100',\n                        help='where to decay lr, can be a list')\n    parser.add_argument('--lr_decay_rate', type=float, default=0.1,\n                        help='decay rate for learning rate')\n    parser.add_argument('--weight_decay', type=float, default=1e-4,\n                        help='weight decay')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        help='momentum')\n    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n    parser.add_argument('--train_image_path', type=str, default='train data csv')\n    parser.add_argument('--test_image_path', type=str, default='test data csv')\n    parser.add_argument('--train_clin_path', type=str, default='train clin csv')\n    parser.add_argument('--test_clin_path', type=str, default='test clin csv')\n    parser.add_argument('--val_csv_path', type=str, default='test clin csv')\n    \n    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n    parser.add_argument('--ncls', type=int, default=6, help='Number of Classes')\n    # model dataset\n    parser.add_argument('--model', type=str, default='resnet50')\n    parser.add_argument('--dataset', type=str, default='TREX_DME',\n                        choices=[ 'OLIVES'], help='dataset')\n    parser.add_argument('--mean', type=str, help='mean of dataset in path in form of str tuple')\n    parser.add_argument('--std', type=str, help='std of dataset in path in form of str tuple')\n    parser.add_argument('--data_folder', type=str, default=None, help='path to custom dataset')\n    parser.add_argument('--size', type=int, default=128, help='parameter for RandomResizedCrop')\n\n    # temperature\n    parser.add_argument('--temp', type=float, default=0.07,\n                        help='temperature for loss function')\n\n\n\n    opt = parser.parse_args(args)\n\n    # check if dataset is path that passed required arguments\n    if opt.dataset == 'path':\n        assert opt.data_folder is not None \\\n               and opt.mean is not None \\\n               and opt.std is not None\n\n    # set the path according to the environment\n    if opt.data_folder is None:\n        opt.data_folder = './datasets/'\n    opt.model_path = './save/{}_models'.format(opt.dataset)\n\n    iterations = opt.lr_decay_epochs.split(',')\n    opt.lr_decay_epochs = list([])\n    for it in iterations:\n        opt.lr_decay_epochs.append(int(it))\n\n    opt.model_name = '{}_lr_{}_decay_{}_bsz_{}_temp_{}'. \\\n        format(opt.model, opt.learning_rate,\n               opt.weight_decay, opt.batch_size, opt.temp)\n\n\n    opt.save_folder = os.path.join(opt.model_path, opt.model_name)\n    if not os.path.isdir(opt.save_folder):\n        os.makedirs(opt.save_folder)\n\n    return opt","metadata":{"papermill":{"duration":0.030788,"end_time":"2023-08-23T17:17:39.800587","exception":false,"start_time":"2023-08-23T17:17:39.769799","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.544887Z","iopub.execute_input":"2023-09-03T06:06:35.545264Z","iopub.status.idle":"2023-09-03T06:06:35.566263Z","shell.execute_reply.started":"2023-09-03T06:06:35.545231Z","shell.execute_reply":"2023-09-03T06:06:35.565281Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"# Not used\n# make a csv file from xlsx\ndef combine_excel(csv_dir):\n    filenames = glob.glob(csv_dir + \"/*.xlsx\")\n    outputxlsx = pd.DataFrame()\n\n    for file in filenames:\n        df = pd.concat(pd.read_excel(file, sheet_name=None), ignore_index=True, sort=False)\n        outputxlsx = outputxlsx.append(df, ignore_index=True)\n\n    outputxlsx.to_csv('test_set_labels.csv',index=False)\n\n# Not used\n# to-do any analyzing on the data frames\ndef analyze_dataframe(csv_dir):\n    pass\n\n# Not used\n# to-do any image processing to simplify the images\ndef process_images(csv_dir):\n    df = pd.read_csv(csv_dir)\n\n    for i in tqdm(range(0,len(df))):\n        path = df.iloc[i,0]\n        im = Image.open(path).convert('L')\n\n# Not used\n# saves the numpy file into a .csv for submission\ndef numpy_submission(sub_dir,np_dir):\n    np_file  = np.load(np_dir)\n    print(len(np_file))\n    sub_dir = pd.read_csv(sub_dir)\n    print(len(sub_dir))\n    for i in range(0,len(sub_dir)):\n        sub_dir.iloc[i,1] = np_file[i,0]\n        sub_dir.iloc[i, 2] = np_file[i, 1]\n        sub_dir.iloc[i, 3] = np_file[i, 2]\n        sub_dir.iloc[i, 4] = np_file[i, 3]\n        sub_dir.iloc[i, 5] = np_file[i, 4]\n        sub_dir.iloc[i, 6] = np_file[i, 5]\n    print(sub_dir.head())\n    sub_dir.to_csv('baseline_result.csv',index=False)","metadata":{"papermill":{"duration":0.020808,"end_time":"2023-08-23T17:17:39.827261","exception":false,"start_time":"2023-08-23T17:17:39.806453","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.567568Z","iopub.execute_input":"2023-09-03T06:06:35.568862Z","iopub.status.idle":"2023-09-03T06:06:35.581497Z","shell.execute_reply.started":"2023-09-03T06:06:35.568828Z","shell.execute_reply":"2023-09-03T06:06:35.580532Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"class OLIVES(data.Dataset):\n    def __init__(self,df, img_dir, clinical_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n        self.clinical_dir = pd.read_csv(clinical_dir)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        temp_path = self.df.iloc[idx,0][1:9]\n        if temp_path == \"TREX DME\":\n            path = self.img_dir + \"/TREX_DME\" + self.df.iloc[idx,0]\n        else:\n            path = self.img_dir  + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        assert self.df.iloc[idx,0] == self.clinical_dir.iloc[idx,0]\n        \n        c1 = (self.clinical_dir.iloc[idx,1] - 27) / 73\n        c2 = (self.clinical_dir.iloc[idx,2] - 151) / 572\n        clinical = torch.tensor([c1, c2])\n        return image, bio_tensor, clinical\n\nclass RECOVERY(data.Dataset):\n    def __init__(self,df, img_dir, transforms):\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        temp_path = self.df.iloc[idx,0][1:9]\n        if temp_path == \"TREX DME\":\n            path = self.img_dir + \"/TREX_DME\" + self.df.iloc[idx,0]\n        else:\n            path = self.img_dir  + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        return image\n\nclass RECOVERY_TEST(data.Dataset):\n    def __init__(self,df, img_dir,clinical_dir, transforms): #\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df = pd.read_csv(df)\n        self.clinical_dir = pd.read_csv(clinical_dir)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        temp_path = self.df.iloc[idx,0][0:9]\n        if temp_path == \"TREX DME\": #changed\n            path = self.img_dir + \"/TREX_DME\" + self.df.iloc[idx,0]\n        else:\n            path = self.img_dir  + self.df.iloc[idx,0]\n        image = Image.open(path).convert(\"L\")\n        image = np.array(image)\n        image = Image.fromarray(image)\n        image = self.transforms(image)\n        b1 = self.df.iloc[idx,1]\n        b2 = self.df.iloc[idx,2]\n        b3 = self.df.iloc[idx,3]\n        b4 = self.df.iloc[idx, 4]\n        b5 = self.df.iloc[idx, 5]\n        b6 = self.df.iloc[idx, 6]\n        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n        \n        assert self.df.iloc[idx,0] == self.clinical_dir.iloc[idx,0]\n        \n        c1 = (self.clinical_dir.iloc[idx,1] - 57) / 39\n        c2 = (self.clinical_dir.iloc[idx,2] - 160) / 166\n\n        clinical = torch.tensor([c1, c2])\n        return image, bio_tensor, clinical","metadata":{"papermill":{"duration":0.034174,"end_time":"2023-08-23T17:17:39.867054","exception":false,"start_time":"2023-08-23T17:17:39.832880","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.584212Z","iopub.execute_input":"2023-09-03T06:06:35.584792Z","iopub.status.idle":"2023-09-03T06:06:35.607382Z","shell.execute_reply.started":"2023-09-03T06:06:35.584760Z","shell.execute_reply":"2023-09-03T06:06:35.606443Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    # define model elements\n    def __init__(self, n_inputs, n_outputs):\n        super(MLP, self).__init__()\n        self.layer = nn.Sequential(\n\n            # for clinical label only baseline\n            nn.Linear(n_inputs, n_inputs),\n            nn.LeakyReLU(0.2, True),\n            nn.Linear(n_inputs, n_outputs)\n        )\n\n    # forward propagate input\n    def forward(self, X):\n        X = self.layer(X)\n        return X","metadata":{"papermill":{"duration":0.014547,"end_time":"2023-08-23T17:17:39.887001","exception":false,"start_time":"2023-08-23T17:17:39.872454","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.608890Z","iopub.execute_input":"2023-09-03T06:06:35.609250Z","iopub.status.idle":"2023-09-03T06:06:35.621184Z","shell.execute_reply.started":"2023-09-03T06:06:35.609216Z","shell.execute_reply":"2023-09-03T06:06:35.619959Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    \"\"\"encoder + classifier\"\"\"\n    def __init__(self, name='resnet50', num_classes=2):\n        super(ResNet, self).__init__()\n\n        self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n        self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.encoder.fc = nn.Identity()\n        self.clin = MLP(2, 8)\n        self.fc = MLP(2048+8, num_classes)\n\n    def forward(self, x, c):\n        encoder_out = self.encoder(x)\n        clinical_out = self.clin(c)\n        # Concatenate along the feature dimension\n        concat_both = torch.cat([encoder_out, clinical_out], dim=1)\n        return self.fc(concat_both)","metadata":{"papermill":{"duration":0.016748,"end_time":"2023-08-23T17:17:39.908598","exception":false,"start_time":"2023-08-23T17:17:39.891850","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.622788Z","iopub.execute_input":"2023-09-03T06:06:35.623492Z","iopub.status.idle":"2023-09-03T06:06:35.632272Z","shell.execute_reply.started":"2023-09-03T06:06:35.623459Z","shell.execute_reply":"2023-09-03T06:06:35.631329Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"# create a model and a criterion (loss function) based on passed arguments opt\ndef set_model(opt):\n    device = opt.device\n    model = ResNet(name=opt.model,num_classes = opt.ncls)\n\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n    return model, criterion\n\n# used in main\ndef set_loader(opt):\n    # construct data loader\n    if opt.dataset == 'OLIVES' or opt.dataset == 'RECOVERY':\n        mean = (.1706)\n        std = (.2112)\n    else:\n        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n\n    normalize = transforms.Normalize(mean=mean, std=std)\n\n    # set of transforms to be performed on the training dataset\n    train_transform = transforms.Compose([\n        transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n        transforms.RandomHorizontalFlip(),\n\n        transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n        ], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.ToTensor(),\n        normalize,\n    ])\n    \n    # set of transforms to be performed on the test dataset\n    val_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    # assign paths and create dataset objects\n    if opt.dataset =='OLIVES':\n        csv_path_train = opt.train_csv_path\n        csv_path_val = opt.val_csv_path\n        csv_path_test = opt.test_csv_path\n        data_path_train = opt.train_image_path\n        data_path_test = opt.test_image_path\n        clin_path_train = opt.train_clin_path\n        clin_path_test = opt.test_clin_path\n        \n        train_dataset = OLIVES(csv_path_train,data_path_train,clin_path_train ,transforms = train_transform)\n        val_dataset = RECOVERY_TEST(csv_path_val,data_path_test, clin_path_test ,transforms = val_transform)\n        test_dataset = RECOVERY_TEST(csv_path_test,data_path_test, clin_path_test ,transforms = val_transform) \n    else:\n        raise ValueError(opt.dataset)\n\n    # define data loaders\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=opt.batch_size, shuffle=True,\n        num_workers=opt.num_workers, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=1, shuffle=False,\n        num_workers=0, pin_memory=True,drop_last=False)\n\n    return train_loader, test_loader, val_loader\n\n# used in main\n# compute and store metrics during training\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n# not used\n# calculate the accuracy of model predictions\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n# not used\n# Dynamically adjust the learning rate while optimizing\ndef adjust_learning_rate(args, optimizer, epoch):\n    lr = args.learning_rate\n    if args.cosine:\n        eta_min = lr * (args.lr_decay_rate ** 3)\n        lr = eta_min + (lr - eta_min) * (\n                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n    else:\n        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n        if steps > 0:\n            lr = lr * (args.lr_decay_rate ** steps)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n# not used\n# gradually increase the learning rate during the initial epochs of training\ndef warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n    if args.warm and epoch <= args.warm_epochs:\n        p = (batch_id + (epoch - 1) * total_batches) / \\\n            (args.warm_epochs * total_batches)\n        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n# used in the end\n# creates and configures optimizer for the model\ndef set_optimizer(opt, model):\n\n    optimizer = optim.SGD(model.parameters(),\n                          lr=opt.learning_rate,\n                          momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n    return optimizer\n\n# Save the trained model's state, along with relevant training information, to a specified file\ndef save_model(model, optimizer, opt, epoch, save_file):\n    print('==> Saving...')\n    state = {\n        'opt': opt,\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(state, save_file)\n    del state","metadata":{"papermill":{"duration":0.033382,"end_time":"2023-08-23T17:17:39.947074","exception":false,"start_time":"2023-08-23T17:17:39.913692","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.633794Z","iopub.execute_input":"2023-09-03T06:06:35.634569Z","iopub.status.idle":"2023-09-03T06:06:35.661506Z","shell.execute_reply.started":"2023-09-03T06:06:35.634537Z","shell.execute_reply":"2023-09-03T06:06:35.660550Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"max_acc=0\nmax_acc_epoch=0\n\n# used in the end and main\ndef train_supervised(train_loader, model,criterion, optimizer, epoch, opt):\n    \"\"\"one epoch training\"\"\"\n    model.train()\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    device = opt.device\n    end = time.time()\n    correct_predictions = 0\n\n    for idx, (image, bio_tensor, clinical) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n\n        images = image.to(device)\n        clinicals = clinical.float().to(device)\n\n        labels = bio_tensor.float()\n\n        labels = labels.to(device)\n        bsz = labels.shape[0]\n\n        # compute loss\n        \n\n        output = model(images, clinicals)\n\n        \n        loss = criterion(output, labels)\n        \n        predicted_labels = torch.round(torch.sigmoid(output))\n        correct_predictions += (predicted_labels == labels).sum().item()\n\n        # update metric\n        losses.update(loss.item(), bsz)\n\n        # SGD\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        total_values = len(train_loader.dataset)*6\n        training_accuracy = (correct_predictions / total_values)*100\n\n        # print info\n        if (idx + 1) % opt.print_freq == 0:\n            print('Train: [{0}][{1}/{2}]\\t'.format(epoch, idx + 1, len(train_loader)))\n            print(f\"Training Accuracy =  {training_accuracy:.2f}%\")\n\n            sys.stdout.flush()\n        \n        global max_acc\n        global max_acc_epoch\n        if training_accuracy > max_acc:\n            max_acc = training_accuracy\n            max_acc_epoch = epoch\n\n    return losses.avg\n\n\n# used in main\n\ndef validate(val_loader, model, criterion, opt):\n    \"\"\"Validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    losses = AverageMeter()\n    label_list = []\n    out_list = []\n\n    with torch.no_grad():\n        for idx, (image, bio_tensor, clinical) in enumerate(val_loader):\n            images = image.to(device)\n            clinicals = clinical.float().to(device)\n            labels = bio_tensor.float()\n            label_list.append(labels.squeeze().detach().cpu().numpy())\n            labels = labels.to(device)\n            bsz = labels.shape[0]\n\n            output = model(images, clinicals)\n            loss = criterion(output, labels)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n            losses.update(loss.item(),bsz)\n    \n    label_array = np.array(label_list)\n    out_array = np.array(out_list)\n    r = f1_score(label_array, out_array, average='macro')\n\n    return losses.avg, r\n\ndef submission_generate(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    with torch.no_grad():\n        for idx, (image,  bio_tensor, clinical) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            clinicals = clinical.float().to(device)\n\n            # forward\n            output = model(images, clinicals)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n\n    out_submisison = np.array(out_list)\n    np.save('output',out_submisison)\n\n# used in main\ndef sample_evaluation(val_loader, model, opt):\n    \"\"\"validation\"\"\"\n    model.eval()\n\n    device = opt.device\n    out_list = []\n    label_list = []\n    with torch.no_grad():\n        for idx, (image, bio_tensor, clinical) in (enumerate(val_loader)):\n\n            images = image.float().to(device)\n            labels = bio_tensor.float()\n\n            labels = labels.float()\n\n            label_list.append(labels.squeeze().detach().cpu().numpy())\n            # forward\n            output = model(images, clinical)\n            output = torch.round(torch.sigmoid(output))\n            out_list.append(output.squeeze().detach().cpu().numpy())\n\n    label_array = np.array(label_list)\n    out_array = np.array(out_list)\n    f = f1_score(label_array,out_array,average='macro')\n    print(f)\n\n\ndef main():\n    # access parse_op function\n    opt = parse_option(args)\n\n    # build data loader\n    train_loader,test_loader, val_loader = set_loader(opt)\n\n    # build model and criterion\n    model, criterion = set_model(opt)\n\n    # build optimizer\n    optimizer = set_optimizer(opt, model)\n\n\n    # training routine\n    for epoch in range(1, 1 + opt.epochs):\n        train_supervised(train_loader, model, criterion, optimizer, epoch, opt)\n        \n        loss, r = validate(val_loader, model, criterion, opt)\n        print('Train epoch {}, f1_score:{:.2f}'.format(epoch, r))\n    \n    print(\"Max accuracy: \", max_acc, \" at epoch: \", max_acc_epoch)\n\n    submission_generate(test_loader, model, opt)\n    #sample_evaluation(test_loader, model, opt)\n\n    save_file = os.path.join(\n        opt.save_folder, 'last.pth')\n    save_model(model, optimizer, opt, opt.epochs, save_file)\n\n    return\n# if __name__ == '__main__':\n    # main()","metadata":{"papermill":{"duration":0.039314,"end_time":"2023-08-23T17:17:39.991595","exception":false,"start_time":"2023-08-23T17:17:39.952281","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.663134Z","iopub.execute_input":"2023-09-03T06:06:35.663828Z","iopub.status.idle":"2023-09-03T06:06:35.691316Z","shell.execute_reply.started":"2023-09-03T06:06:35.663795Z","shell.execute_reply":"2023-09-03T06:06:35.690320Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"#/kaggle/input/d/salmankhondker/olives-vip-cup-2023/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST","metadata":{"papermill":{"duration":0.01641,"end_time":"2023-08-23T17:17:40.012892","exception":false,"start_time":"2023-08-23T17:17:39.996482","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.692488Z","iopub.execute_input":"2023-09-03T06:06:35.694249Z","iopub.status.idle":"2023-09-03T06:06:35.704534Z","shell.execute_reply.started":"2023-09-03T06:06:35.694179Z","shell.execute_reply":"2023-09-03T06:06:35.703601Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"args = ['--batch_size', '128', '--model', \"resnet50\", '--dataset', 'OLIVES', '--epochs', '25', '--device', 'cuda:0', \n        '--train_image_path', '/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES', \n        '--test_image_path', '/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/', \n        '--test_csv_path', '/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv',\n        '--train_csv_path', '/kaggle/input/modified-training-biomarkers/Modified training biomarkers.csv', \n        '--train_clin_path' , '/kaggle/input/clinical/Modified training clinical.csv' ,\n        '--test_clin_path', '/kaggle/input/clinical/Modified test clinical.csv',\n        '--val_csv_path', '/kaggle/input/test-dataset/Leaked answers.csv']","metadata":{"papermill":{"duration":0.015375,"end_time":"2023-08-23T17:17:40.033199","exception":false,"start_time":"2023-08-23T17:17:40.017824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.708074Z","iopub.execute_input":"2023-09-03T06:06:35.708336Z","iopub.status.idle":"2023-09-03T06:06:35.716082Z","shell.execute_reply.started":"2023-09-03T06:06:35.708313Z","shell.execute_reply":"2023-09-03T06:06:35.714998Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"now_lets_run_our_code = main()","metadata":{"papermill":{"duration":9027.589473,"end_time":"2023-08-23T19:48:07.646052","exception":false,"start_time":"2023-08-23T17:17:40.056579","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:06:35.717620Z","iopub.execute_input":"2023-09-03T06:06:35.718281Z","iopub.status.idle":"2023-09-03T06:13:59.230028Z","shell.execute_reply.started":"2023-09-03T06:06:35.718250Z","shell.execute_reply":"2023-09-03T06:13:59.228575Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Training Accuracy =  0.73%\nTraining Accuracy =  1.67%\nTraining Accuracy =  2.56%\nTraining Accuracy =  3.42%\nTraining Accuracy =  4.26%\nTraining Accuracy =  5.11%\nTraining Accuracy =  6.02%\nTraining Accuracy =  6.91%\nTraining Accuracy =  7.78%\nTrain: [1][10/73]\t\nTraining Accuracy =  8.66%\nTraining Accuracy =  9.51%\nTraining Accuracy =  10.41%\nTraining Accuracy =  11.34%\nTraining Accuracy =  12.21%\nTraining Accuracy =  13.09%\nTraining Accuracy =  13.97%\nTraining Accuracy =  14.84%\nTraining Accuracy =  15.70%\nTraining Accuracy =  16.59%\nTrain: [1][20/73]\t\nTraining Accuracy =  17.48%\nTraining Accuracy =  18.39%\nTraining Accuracy =  19.29%\nTraining Accuracy =  20.18%\nTraining Accuracy =  21.07%\nTraining Accuracy =  21.95%\nTraining Accuracy =  22.86%\nTraining Accuracy =  23.80%\nTraining Accuracy =  24.67%\nTraining Accuracy =  25.54%\nTrain: [1][30/73]\t\nTraining Accuracy =  26.44%\nTraining Accuracy =  27.33%\nTraining Accuracy =  28.24%\nTraining Accuracy =  29.11%\nTraining Accuracy =  29.96%\nTraining Accuracy =  30.85%\nTraining Accuracy =  31.73%\nTraining Accuracy =  32.62%\nTraining Accuracy =  33.55%\nTraining Accuracy =  34.46%\nTrain: [1][40/73]\t\nTraining Accuracy =  35.36%\nTraining Accuracy =  36.29%\nTraining Accuracy =  37.23%\nTraining Accuracy =  38.16%\nTraining Accuracy =  39.06%\nTraining Accuracy =  39.91%\nTraining Accuracy =  40.76%\nTraining Accuracy =  41.63%\nTraining Accuracy =  42.50%\nTraining Accuracy =  43.42%\nTrain: [1][50/73]\t\nTraining Accuracy =  44.36%\nTraining Accuracy =  45.26%\nTraining Accuracy =  46.10%\nTraining Accuracy =  47.01%\nTraining Accuracy =  47.92%\nTraining Accuracy =  48.83%\nTraining Accuracy =  49.77%\nTraining Accuracy =  50.67%\nTraining Accuracy =  51.58%\nTraining Accuracy =  52.47%\nTrain: [1][60/73]\t\nTraining Accuracy =  53.39%\nTraining Accuracy =  54.29%\nTraining Accuracy =  55.19%\nTraining Accuracy =  56.04%\nTraining Accuracy =  56.91%\nTraining Accuracy =  57.80%\nTraining Accuracy =  58.71%\nTraining Accuracy =  59.60%\nTraining Accuracy =  60.49%\nTraining Accuracy =  61.42%\nTrain: [1][70/73]\t\nTraining Accuracy =  62.38%\nTraining Accuracy =  63.28%\nTraining Accuracy =  64.20%\nTraining Accuracy =  64.87%\nTrain epoch 1, f1_score:0.22\nTraining Accuracy =  0.95%\nTraining Accuracy =  1.86%\nTraining Accuracy =  2.75%\nTraining Accuracy =  3.64%\nTraining Accuracy =  4.56%\nTraining Accuracy =  5.46%\nTraining Accuracy =  6.33%\nTraining Accuracy =  7.21%\nTraining Accuracy =  8.12%\nTrain: [2][10/73]\t\nTraining Accuracy =  9.03%\nTraining Accuracy =  9.93%\nTraining Accuracy =  10.86%\nTraining Accuracy =  11.77%\nTraining Accuracy =  12.69%\nTraining Accuracy =  13.59%\nTraining Accuracy =  14.51%\nTraining Accuracy =  15.40%\nTraining Accuracy =  16.31%\nTraining Accuracy =  17.23%\nTrain: [2][20/73]\t\nTraining Accuracy =  18.17%\nTraining Accuracy =  19.06%\nTraining Accuracy =  19.99%\nTraining Accuracy =  20.91%\nTraining Accuracy =  21.85%\nTraining Accuracy =  22.75%\nTraining Accuracy =  23.66%\nTraining Accuracy =  24.56%\nTraining Accuracy =  25.46%\nTraining Accuracy =  26.37%\nTrain: [2][30/73]\t\nTraining Accuracy =  27.31%\nTraining Accuracy =  28.23%\nTraining Accuracy =  29.15%\nTraining Accuracy =  30.06%\nTraining Accuracy =  31.00%\nTraining Accuracy =  31.89%\nTraining Accuracy =  32.75%\nTraining Accuracy =  33.67%\nTraining Accuracy =  34.62%\nTraining Accuracy =  35.51%\nTrain: [2][40/73]\t\nTraining Accuracy =  36.41%\nTraining Accuracy =  37.32%\nTraining Accuracy =  38.22%\nTraining Accuracy =  39.20%\nTraining Accuracy =  40.10%\nTraining Accuracy =  41.03%\nTraining Accuracy =  41.95%\nTraining Accuracy =  42.89%\nTraining Accuracy =  43.80%\nTraining Accuracy =  44.75%\nTrain: [2][50/73]\t\nTraining Accuracy =  45.65%\nTraining Accuracy =  46.60%\nTraining Accuracy =  47.51%\nTraining Accuracy =  48.45%\nTraining Accuracy =  49.37%\nTraining Accuracy =  50.33%\nTraining Accuracy =  51.27%\nTraining Accuracy =  52.24%\nTraining Accuracy =  53.15%\nTraining Accuracy =  54.09%\nTrain: [2][60/73]\t\nTraining Accuracy =  54.97%\nTraining Accuracy =  55.89%\nTraining Accuracy =  56.82%\nTraining Accuracy =  57.74%\nTraining Accuracy =  58.71%\nTraining Accuracy =  59.63%\nTraining Accuracy =  60.56%\nTraining Accuracy =  61.54%\nTraining Accuracy =  62.47%\nTraining Accuracy =  63.42%\nTrain: [2][70/73]\t\nTraining Accuracy =  64.31%\nTraining Accuracy =  65.21%\nTraining Accuracy =  66.09%\nTraining Accuracy =  66.76%\nTrain epoch 2, f1_score:0.23\nTraining Accuracy =  0.93%\nTraining Accuracy =  1.85%\nTraining Accuracy =  2.80%\nTraining Accuracy =  3.66%\nTraining Accuracy =  4.60%\nTraining Accuracy =  5.54%\nTraining Accuracy =  6.51%\nTraining Accuracy =  7.38%\nTraining Accuracy =  8.28%\nTrain: [3][10/73]\t\nTraining Accuracy =  9.23%\nTraining Accuracy =  10.16%\nTraining Accuracy =  11.10%\nTraining Accuracy =  12.04%\nTraining Accuracy =  12.93%\nTraining Accuracy =  13.89%\nTraining Accuracy =  14.86%\nTraining Accuracy =  15.77%\nTraining Accuracy =  16.68%\nTraining Accuracy =  17.58%\nTrain: [3][20/73]\t\nTraining Accuracy =  18.50%\nTraining Accuracy =  19.45%\nTraining Accuracy =  20.35%\nTraining Accuracy =  21.34%\nTraining Accuracy =  22.30%\nTraining Accuracy =  23.21%\nTraining Accuracy =  24.15%\nTraining Accuracy =  25.08%\nTraining Accuracy =  25.96%\nTraining Accuracy =  26.88%\nTrain: [3][30/73]\t\nTraining Accuracy =  27.84%\nTraining Accuracy =  28.74%\nTraining Accuracy =  29.67%\nTraining Accuracy =  30.64%\nTraining Accuracy =  31.55%\nTraining Accuracy =  32.47%\nTraining Accuracy =  33.40%\nTraining Accuracy =  34.31%\nTraining Accuracy =  35.25%\nTraining Accuracy =  36.12%\nTrain: [3][40/73]\t\nTraining Accuracy =  37.05%\nTraining Accuracy =  37.94%\nTraining Accuracy =  38.87%\nTraining Accuracy =  39.81%\nTraining Accuracy =  40.76%\nTraining Accuracy =  41.64%\nTraining Accuracy =  42.60%\nTraining Accuracy =  43.58%\nTraining Accuracy =  44.55%\nTraining Accuracy =  45.52%\nTrain: [3][50/73]\t\nTraining Accuracy =  46.44%\nTraining Accuracy =  47.41%\nTraining Accuracy =  48.37%\nTraining Accuracy =  49.30%\nTraining Accuracy =  50.21%\nTraining Accuracy =  51.15%\nTraining Accuracy =  52.12%\nTraining Accuracy =  53.09%\nTraining Accuracy =  54.02%\nTraining Accuracy =  54.97%\nTrain: [3][60/73]\t\nTraining Accuracy =  55.89%\nTraining Accuracy =  56.82%\nTraining Accuracy =  57.77%\nTraining Accuracy =  58.70%\nTraining Accuracy =  59.63%\nTraining Accuracy =  60.54%\nTraining Accuracy =  61.49%\nTraining Accuracy =  62.37%\nTraining Accuracy =  63.32%\nTraining Accuracy =  64.21%\nTrain: [3][70/73]\t\nTraining Accuracy =  65.12%\nTraining Accuracy =  66.03%\nTraining Accuracy =  66.96%\nTraining Accuracy =  67.63%\nTrain epoch 3, f1_score:0.26\nTraining Accuracy =  0.91%\nTraining Accuracy =  1.83%\nTraining Accuracy =  2.75%\nTraining Accuracy =  3.66%\nTraining Accuracy =  4.63%\nTraining Accuracy =  5.58%\nTraining Accuracy =  6.51%\nTraining Accuracy =  7.42%\nTraining Accuracy =  8.35%\nTrain: [4][10/73]\t\nTraining Accuracy =  9.24%\nTraining Accuracy =  10.18%\nTraining Accuracy =  11.14%\nTraining Accuracy =  12.08%\nTraining Accuracy =  13.01%\nTraining Accuracy =  13.95%\nTraining Accuracy =  14.89%\nTraining Accuracy =  15.80%\nTraining Accuracy =  16.70%\nTraining Accuracy =  17.64%\nTrain: [4][20/73]\t\nTraining Accuracy =  18.53%\nTraining Accuracy =  19.44%\nTraining Accuracy =  20.34%\nTraining Accuracy =  21.26%\nTraining Accuracy =  22.21%\nTraining Accuracy =  23.15%\nTraining Accuracy =  24.12%\nTraining Accuracy =  25.06%\nTraining Accuracy =  26.02%\nTraining Accuracy =  26.96%\nTrain: [4][30/73]\t\nTraining Accuracy =  27.92%\nTraining Accuracy =  28.89%\nTraining Accuracy =  29.82%\nTraining Accuracy =  30.76%\nTraining Accuracy =  31.70%\nTraining Accuracy =  32.67%\nTraining Accuracy =  33.59%\nTraining Accuracy =  34.58%\nTraining Accuracy =  35.50%\nTraining Accuracy =  36.43%\nTrain: [4][40/73]\t\nTraining Accuracy =  37.38%\nTraining Accuracy =  38.30%\nTraining Accuracy =  39.20%\nTraining Accuracy =  40.09%\nTraining Accuracy =  41.06%\nTraining Accuracy =  41.98%\nTraining Accuracy =  42.94%\nTraining Accuracy =  43.91%\nTraining Accuracy =  44.83%\nTraining Accuracy =  45.78%\nTrain: [4][50/73]\t\nTraining Accuracy =  46.70%\nTraining Accuracy =  47.65%\nTraining Accuracy =  48.55%\nTraining Accuracy =  49.45%\nTraining Accuracy =  50.38%\nTraining Accuracy =  51.34%\nTraining Accuracy =  52.29%\nTraining Accuracy =  53.22%\nTraining Accuracy =  54.16%\nTraining Accuracy =  55.10%\nTrain: [4][60/73]\t\nTraining Accuracy =  56.03%\nTraining Accuracy =  56.99%\nTraining Accuracy =  57.94%\nTraining Accuracy =  58.86%\nTraining Accuracy =  59.81%\nTraining Accuracy =  60.73%\nTraining Accuracy =  61.66%\nTraining Accuracy =  62.59%\nTraining Accuracy =  63.46%\nTraining Accuracy =  64.41%\nTrain: [4][70/73]\t\nTraining Accuracy =  65.35%\nTraining Accuracy =  66.32%\nTraining Accuracy =  67.30%\nTraining Accuracy =  67.96%\nTrain epoch 4, f1_score:0.26\nTraining Accuracy =  0.98%\nTraining Accuracy =  1.92%\nTraining Accuracy =  2.92%\nTraining Accuracy =  3.83%\nTraining Accuracy =  4.74%\nTraining Accuracy =  5.69%\nTraining Accuracy =  6.59%\nTraining Accuracy =  7.51%\nTraining Accuracy =  8.43%\nTrain: [5][10/73]\t\nTraining Accuracy =  9.36%\nTraining Accuracy =  10.31%\nTraining Accuracy =  11.27%\nTraining Accuracy =  12.22%\nTraining Accuracy =  13.18%\nTraining Accuracy =  14.12%\nTraining Accuracy =  15.05%\nTraining Accuracy =  15.97%\nTraining Accuracy =  16.91%\nTraining Accuracy =  17.86%\nTrain: [5][20/73]\t\nTraining Accuracy =  18.80%\nTraining Accuracy =  19.76%\nTraining Accuracy =  20.62%\nTraining Accuracy =  21.55%\nTraining Accuracy =  22.49%\nTraining Accuracy =  23.42%\nTraining Accuracy =  24.35%\nTraining Accuracy =  25.27%\nTraining Accuracy =  26.21%\nTraining Accuracy =  27.18%\nTrain: [5][30/73]\t\nTraining Accuracy =  28.10%\nTraining Accuracy =  29.04%\nTraining Accuracy =  29.98%\nTraining Accuracy =  30.95%\nTraining Accuracy =  31.92%\nTraining Accuracy =  32.85%\nTraining Accuracy =  33.78%\nTraining Accuracy =  34.73%\nTraining Accuracy =  35.67%\nTraining Accuracy =  36.60%\nTrain: [5][40/73]\t\nTraining Accuracy =  37.50%\nTraining Accuracy =  38.47%\nTraining Accuracy =  39.42%\nTraining Accuracy =  40.40%\nTraining Accuracy =  41.36%\nTraining Accuracy =  42.34%\nTraining Accuracy =  43.27%\nTraining Accuracy =  44.20%\nTraining Accuracy =  45.16%\nTraining Accuracy =  46.09%\nTrain: [5][50/73]\t\nTraining Accuracy =  47.06%\nTraining Accuracy =  48.05%\nTraining Accuracy =  48.98%\nTraining Accuracy =  49.91%\nTraining Accuracy =  50.86%\nTraining Accuracy =  51.78%\nTraining Accuracy =  52.71%\nTraining Accuracy =  53.67%\nTraining Accuracy =  54.59%\nTraining Accuracy =  55.55%\nTrain: [5][60/73]\t\nTraining Accuracy =  56.50%\nTraining Accuracy =  57.41%\nTraining Accuracy =  58.36%\nTraining Accuracy =  59.32%\nTraining Accuracy =  60.25%\nTraining Accuracy =  61.24%\nTraining Accuracy =  62.16%\nTraining Accuracy =  63.16%\nTraining Accuracy =  64.05%\nTraining Accuracy =  64.95%\nTrain: [5][70/73]\t\nTraining Accuracy =  65.91%\nTraining Accuracy =  66.85%\nTraining Accuracy =  67.81%\nTraining Accuracy =  68.51%\nTrain epoch 5, f1_score:0.21\nTraining Accuracy =  0.95%\nTraining Accuracy =  1.87%\nTraining Accuracy =  2.84%\nTraining Accuracy =  3.76%\nTraining Accuracy =  4.74%\nTraining Accuracy =  5.68%\nTraining Accuracy =  6.67%\nTraining Accuracy =  7.57%\nTraining Accuracy =  8.46%\nTrain: [6][10/73]\t\nTraining Accuracy =  9.39%\nTraining Accuracy =  10.32%\nTraining Accuracy =  11.25%\nTraining Accuracy =  12.25%\nTraining Accuracy =  13.23%\nTraining Accuracy =  14.22%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[144], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m now_lets_run_our_code \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[141], line 169\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# training routine\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m opt\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 169\u001b[0m     \u001b[43mtrain_supervised\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     loss, r \u001b[38;5;241m=\u001b[39m validate(val_loader, model, criterion, opt)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, f1_score:\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, r))\n","Cell \u001b[0;32mIn[141], line 37\u001b[0m, in \u001b[0;36mtrain_supervised\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, opt)\u001b[0m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     36\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(output))\n\u001b[0;32m---> 37\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# update metric\u001b[39;00m\n\u001b[1;32m     40\u001b[0m losses\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), bsz)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"z = numpy_submission('/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv','/kaggle/working/output.npy')","metadata":{"papermill":{"duration":4.05868,"end_time":"2023-08-23T19:48:15.487138","exception":false,"start_time":"2023-08-23T19:48:11.428458","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-03T06:13:59.231470Z","iopub.status.idle":"2023-09-03T06:13:59.232707Z","shell.execute_reply.started":"2023-09-03T06:13:59.232437Z","shell.execute_reply":"2023-09-03T06:13:59.232460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tb","metadata":{"execution":{"iopub.status.busy":"2023-09-03T06:13:59.233974Z","iopub.status.idle":"2023-09-03T06:13:59.234917Z","shell.execute_reply.started":"2023-09-03T06:13:59.234669Z","shell.execute_reply":"2023-09-03T06:13:59.234692Z"},"trusted":true},"execution_count":null,"outputs":[]}]}