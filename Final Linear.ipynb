{"cells":[{"cell_type":"code","execution_count":234,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-02T19:46:04.500867Z","iopub.status.busy":"2023-09-02T19:46:04.500407Z","iopub.status.idle":"2023-09-02T19:46:04.512871Z","shell.execute_reply":"2023-09-02T19:46:04.511925Z","shell.execute_reply.started":"2023-09-02T19:46:04.500826Z"},"papermill":{"duration":0.02368,"end_time":"2023-08-24T16:44:05.741240","exception":false,"start_time":"2023-08-24T16:44:05.717560","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import argparse\n","import math\n","import torch\n","import sys\n","import time\n","import datetime\n","from sklearn.metrics import average_precision_score,roc_auc_score, classification_report\n","from __future__ import print_function\n","import torch.optim as optim\n","import os\n","from sklearn.metrics import roc_auc_score, f1_score\n","import torch.backends.cudnn as cudnn\n","from torchvision import transforms, datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","import torchvision\n","import torch.utils.data as data\n","from PIL import Image\n","import sys\n","from sklearn.metrics import precision_score,recall_score\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Arguments**"]},{"cell_type":"code","execution_count":235,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.515392Z","iopub.status.busy":"2023-09-02T19:46:04.514790Z","iopub.status.idle":"2023-09-02T19:46:04.554864Z","shell.execute_reply":"2023-09-02T19:46:04.553592Z","shell.execute_reply.started":"2023-09-02T19:46:04.515361Z"},"papermill":{"duration":0.034306,"end_time":"2023-08-24T16:44:05.781944","exception":false,"start_time":"2023-08-24T16:44:05.747638","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["### from config.config_linear import parse_option ###\n","\n","\n","def parse_option(args):\n","    parser = argparse.ArgumentParser('argument for training')\n","\n","    parser.add_argument('--print_freq', type=int, default=10,\n","                        help='print frequency')\n","    parser.add_argument('--save_freq', type=int, default=50,\n","                        help='save frequency')\n","    parser.add_argument('--batch_size', type=int, default=128,\n","                        help='batch_size')\n","    parser.add_argument('--num_workers', type=int, default=2,\n","                        help='num of workers to use')\n","    parser.add_argument('--epochs', type=int, default=25,\n","                        help='number of training epochs')\n","    parser.add_argument('--n_cls', type=int, default=2,\n","                        help='number of training epochs')\n","    parser.add_argument('--super', type=int, default=0,\n","                        help='number of training epochs')\n","    parser.add_argument('--type', type=int, default=0,\n","                        help='number of training epochs')\n","    parser.add_argument('--biomarker', type=str, default='fluid_irf')\n","\n","    # optimization\n","    parser.add_argument('--learning_rate', type=float, default=0.005,\n","                        help='learning rate')\n","    parser.add_argument('--patient_lambda', type=float, default=1,\n","                        help='learning rate')\n","    parser.add_argument('--lr_decay_epochs', type=str, default=\"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25\",\n","                        help='where to decay lr, can be a list')\n","#     \"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25\"\n","    parser.add_argument('--lr_decay_rate', type=float, default=0.95,\n","                        help='decay rate for learning rate')\n","    parser.add_argument('--weight_decay', type=float, default=0,\n","                        help='weight decay')\n","    parser.add_argument('--momentum', type=float, default=0.9,\n","                        help='momentum')\n","    parser.add_argument('--device', type=str, default='cuda:0')\n","    parser.add_argument('--parallel', type=int, default=1, help='data parallel')\n","    # model dataset\n","    parser.add_argument('--model', type=str, default='resnet50')\n","    parser.add_argument('--train_csv_path', type=str, default='train data csv')\n","    parser.add_argument('--test_csv_path', type=str, default='test data csv')\n","    parser.add_argument('--train_image_path', type=str, default='/data/Datasets')\n","    parser.add_argument('--test_image_path', type=str, default='/data/Datasets')\n","    parser.add_argument('--val_csv_path', type=str, default='val data csv')\n","    parser.add_argument('--val_image_path', type=str, default='val data image')\n","    parser.add_argument('--results_dir_contrastive', type=str, default='/home/kiran/Desktop/Dev/SupCon_OCT_Clinical/results.txt')\n","    parser.add_argument('--img_dir', type=str, default='image directory')\n","    parser.add_argument('--model_type', type=str, default='bcva')\n","    parser.add_argument('--multi', type=int, default=0)\n","    parser.add_argument('--noise_analysis', type=int, default=0)\n","    parser.add_argument('--severity_analysis', type=int, default=0)\n","    parser.add_argument('--dataset', type=str, default='Prime',\n","                        choices=['OCT','Biomarker','Prime'], help='dataset')\n","\n","    # other setting\n","    parser.add_argument('--cosine', action='store_true',\n","                        help='using cosine annealing')\n","    parser.add_argument('--warm', action='store_true',\n","                        help='warm-up for large batch training')\n","    parser.add_argument('--ford_region',type = int,default = 0,\n","                        help='Training on 6 region classes or not')\n","    parser.add_argument('--percentage', type=int, default=100,\n","                        help='Percentage of Biomarker Training Data Utilized')\n","\n","    parser.add_argument('--ckpt', type=str, default='',\n","                        help='path to pre-trained model')\n","    parser.add_argument('--backbone_training', type=str, default='BCVA',\n","                        help='manner in which backbone was trained')\n","    parser.add_argument('--patient_split', type=int, default=1,\n","                        help='choose method')\n","    opt = parser.parse_args(args)\n","\n","    # set the path according to the environment\n","    opt.data_folder = './datasets/'\n","\n","    iterations = opt.lr_decay_epochs.split(',')\n","    opt.lr_decay_epochs = list([])\n","    for it in iterations:\n","        opt.lr_decay_epochs.append(int(it))\n","\n","    opt.model_name = '{}_{}_lr_{}_decay_{}_bsz_{}'.\\\n","        format(opt.dataset, opt.model, opt.learning_rate, opt.weight_decay,\n","               opt.batch_size)\n","\n","    if opt.cosine:\n","        opt.model_name = '{}_cosine'.format(opt.model_name)\n","\n","    # warm-up for large-batch training,\n","    if opt.warm:\n","        opt.model_name = '{}_warm'.format(opt.model_name)\n","        opt.warmup_from = 0.01\n","        opt.warm_epochs = 10\n","        if opt.cosine:\n","            eta_min = opt.learning_rate * (opt.lr_decay_rate ** 3)\n","            opt.warmup_to = eta_min + (opt.learning_rate - eta_min) * (\n","                    1 + math.cos(math.pi * opt.warm_epochs / opt.epochs)) / 2\n","        else:\n","            opt.warmup_to = opt.learning_rate\n","\n","    if opt.dataset == 'cifar10':\n","        opt.n_cls = 10\n","    elif opt.dataset == 'cifar100':\n","        opt.n_cls = 100\n","    elif opt.dataset == 'Ford':\n","        opt.n_cls = 3\n","    elif opt.dataset == 'Ford_Region':\n","        opt.n_cls = 3\n","    elif opt.dataset == 'covid_kaggle':\n","        opt.n_cls = 4\n","    elif opt.dataset == 'qu_dataset':\n","        opt.n_cls = 3\n","    elif opt.dataset == 'covid_x':\n","        opt.n_cls = 2\n","    elif opt.dataset == 'covid_x_A':\n","        opt.n_cls = 3\n","    elif opt.dataset == 'OCT':\n","        opt.n_cls = 4\n","    elif opt.dataset == 'Prime':\n","        opt.n_cls = 2\n","    else:\n","        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n","\n","    return opt"]},{"cell_type":"markdown","metadata":{},"source":["* # Training Main"]},{"cell_type":"code","execution_count":236,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.621139Z","iopub.status.busy":"2023-09-02T19:46:04.618967Z","iopub.status.idle":"2023-09-02T19:46:04.668139Z","shell.execute_reply":"2023-09-02T19:46:04.667083Z","shell.execute_reply.started":"2023-09-02T19:46:04.621106Z"},"trusted":true},"outputs":[],"source":["def main():\n","    best_acc = 0\n","    opt = parse_option(args)\n","\n","    # build data loader\n","    device = opt.device\n","    train_loader,  test_loader = set_loader_new(opt)\n","    val_loader = val_loader_fun(opt)\n","\n","    acc_list = []\n","    prec_list = []\n","    rec_list = []\n","    spec_list = []\n","    # training routine\n","    for i in range(0,1):\n","        r_list = []\n","        model, classifier, criterion = set_model(opt)\n","        optimizer = set_optimizer(opt, classifier)\n","        \n","        for epoch in range(1, opt.epochs + 1):\n","            adjust_learning_rate(opt, optimizer, epoch)\n","\n","            # train for one epoch\n","            time1 = time.time()\n","            loss, acc = train_OCT(train_loader, model, classifier, criterion,\n","                              optimizer, epoch, opt)\n","            time2 = time.time()\n","#             print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(\n","#                 epoch, time2 - time1, acc))\n","\n","            loss, test_acc,prec,rec,spec, r = validate(test_loader, model, classifier, criterion, opt)\n","            print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}, f1_score:{:.4f}'.format(epoch, time2 - time1, acc, r))\n","            \n","            r_list.append(r)\n","        \n","        print(r_list)\n","        \n","        acc_list.append(acc)\n","        prec_list.append(prec)\n","        rec_list.append(rec)\n","        spec_list.append(spec)\n","    df = pd.DataFrame({'Accuracy':acc_list,'Precision':prec_list,'Recall':rec_list,'Specificity':spec_list})\n","    excel_name = opt.backbone_training + '_' + opt.biomarker + '_' + str(opt.patient_split) + '.csv'\n","    df.to_csv(excel_name, index=False)\n","    \n","    submission_generate_single(val_loader, model,classifier, opt)\n","\n","def train_OCT(train_loader, model, classifier, criterion, optimizer, epoch, opt):\n","    \"\"\"one epoch training\"\"\"\n","    model.eval()\n","    classifier.train()\n","\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    device = opt.device\n","    end = time.time()\n","    for idx, (image, vit_deb,ir_hrf, full_vit,partial_vit,fluid_irf,drt,eye_id,bcva,cst,patient) in enumerate(train_loader):\n","        data_time.update(time.time() - end)\n","\n","        images = image.to(device)\n","\n","        if (opt.biomarker == 'vit_deb'):\n","            labels = vit_deb\n","        elif (opt.biomarker == 'ir_hrf'):\n","            labels = ir_hrf\n","        elif (opt.biomarker == 'full_vit'):\n","            labels = full_vit\n","        elif (opt.biomarker == 'partial_vit'):\n","            labels = partial_vit\n","        elif (opt.biomarker == 'drt'):\n","            labels = drt\n","        else:\n","            labels = fluid_irf\n","            \n","#         labels = labels.long()\n","#         labels = torch.tensor([labels])\n","        labels = labels.float()\n","        bsz = labels.shape[0]\n","        labels=labels.to(device)\n","        \n","        # warm-up learning rate\n","        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n","\n","        # compute loss\n","        with torch.no_grad():\n","            features = model.encoder(images)\n","\n","        output = classifier(features.detach())\n","   #     probabilities = torch.sigmoid(output)\n","        \n","#         output = output.squeeze()\n","   #     print(\"output\",output.shape,\"labels\",labels.shape)\n","    #    print(\"Outputs =\",probabilities)\n","     #   print(\"Labels =\",labels)\n","\n","        loss = criterion(output, labels)\n","\n","        # update metric\n","        losses.update(loss.item(), bsz)\n","\n","        acc1= accuracy_single(output, labels, topk=(1,))\n","\n","\n","        top1.update(acc1[0].item(), bsz)\n","\n","        # SGD\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        # print info\n","        if (idx + 1) % opt.print_freq == 0:\n","            print('Train: [{0}][{1}/{2}]\\t'\n","                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n","                  'loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n","                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n","                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n","                   data_time=data_time, loss=losses, top1=top1))\n","            sys.stdout.flush()\n","\n","    return losses.avg, top1.avg\n","\n","\n","def validate(val_loader, model, classifier, criterion, opt):\n","    \"\"\"validation\"\"\"\n","    model.eval()\n","    classifier.eval()\n","    device = opt.device\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    label_list = []\n","    out_list = []\n","    with torch.no_grad():\n","        end = time.time()\n","        for idx, (image, vit_deb,ir_hrf, full_vit,partial_vit,fluid_irf,drt) in enumerate(val_loader):\n","            images = image.float().to(device)\n","\n","            if (opt.biomarker == 'vit_deb'):\n","                labels = vit_deb\n","            elif (opt.biomarker == 'ir_hrf'):\n","                labels = ir_hrf\n","            elif (opt.biomarker == 'full_vit'):\n","                labels = full_vit\n","            elif (opt.biomarker == 'partial_vit'):\n","                labels = partial_vit\n","            elif (opt.biomarker == 'drt'):\n","                labels = drt\n","            else:\n","                labels = fluid_irf\n","                \n","#             labels = torch.tensor([labels])\n","#             labels = labels.long()\n","            labels = labels.float()\n","            label_list.append(labels.squeeze().detach().cpu().numpy())\n","            labels = labels.to(device)\n","            bsz = labels.shape[0]\n","\n","            # forward\n","            output = torch.round(classifier(model.encoder(images)))\n","#             output = output.squeeze()\n","\n","            loss = criterion(output, labels)\n","#             _, pred = output.topk(1, 1, True, True)\n","#             output = torch.round(torch.sigmoid(output))\n","\n","            out_list.append(output.squeeze().detach().cpu().numpy())\n","            # update metri  c\n","            losses.update(loss.item(), bsz)\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","#             if idx % opt.print_freq == 0:\n","#                 print('Test: [{0}/{1}]\\t'\n","#                       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n","#                       'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","#                       'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n","#                        idx, len(val_loader), batch_time=batch_time,\n","#                        loss=losses, top1=top1))\n","\n","#     label_list = np.squeeze(label_list, axis=(2,))\n","    label_array = np.array(label_list)\n","\n","    out_array = np.array(out_list)\n","    \n","#     out_array = np.concatenate(out_list, axis=0)\n","#     print(\"output\",out_array[0:10])\n","#     print(\"labels\",label_array[0:10])\n","    print('zeros', (out_array==np.zeros(out_array.shape)).all(), 'ones', (out_array==np.ones(out_array.shape)).all())\n","    \n","    r = f1_score(label_array.flatten(), out_array.flatten(), average='macro')\n","    #     r = f1_score(label_array, out_array, average='macro')\n","    \n","    prec = precision_score(label_array.flatten(), out_array.flatten())\n","    rec = recall_score(label_array.flatten(), out_array.flatten())\n","    spec = recall_score(label_array.flatten(), out_array.flatten(), pos_label=0)\n","\n","    return losses.avg, top1.avg,prec, rec, spec, r"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Training Multi**"]},{"cell_type":"code","execution_count":237,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.675744Z","iopub.status.busy":"2023-09-02T19:46:04.673610Z","iopub.status.idle":"2023-09-02T19:46:04.707620Z","shell.execute_reply":"2023-09-02T19:46:04.706740Z","shell.execute_reply.started":"2023-09-02T19:46:04.675711Z"},"papermill":{"duration":5.428817,"end_time":"2023-08-24T16:44:11.217012","exception":false,"start_time":"2023-08-24T16:44:05.788195","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["### from training_linear.training_one_epoch_ckpt_multi import main_multilabel ###\n","### main_multilabel ###\n","\n","\n","def train_OCT_multilabel(train_loader, model, classifier, criterion, optimizer, epoch, opt):\n","    \"\"\"one epoch training\"\"\"\n","    model.eval()\n","    classifier.train()\n","\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    device = opt.device\n","    end = time.time()\n","    \n","#     for idx, (image, bio_tensor,eye_id,bcva,cst,patient) in enumerate(train_loader):\n","    for idx, (image, bio_tensor) in enumerate(train_loader):\n","        data_time.update(time.time() - end)\n","\n","        images = image.to(device)\n","\n","        labels = bio_tensor\n","        labels = labels.float()\n","        bsz = labels.shape[0]\n","        labels=labels.to(device)\n","        \n","        # warm-up learning rate\n","        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)\n","\n","        # compute loss\n","        with torch.no_grad():\n","            features = model.encoder(images)\n","\n","        output = classifier(features.detach())\n","#         print(\"Outputs =\",output)\n","#         print(\"Labels =\",labels)\n","        \n","        loss = criterion(output, labels)\n","\n","        # update metric\n","        losses.update(loss.item(), bsz)\n","        acc1= accuracy(output, labels, topk=(1,))\n","        top1.update(acc1[0].item(), bsz)\n","\n","        # SGD\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        # print info\n","        if (idx + 1) % opt.print_freq == 0:\n","            print('Train: [{0}][{1}/{2}]\\t'.format(\n","                epoch, idx + 1, len(train_loader)))\n","            sys.stdout.flush()\n","\n","    return losses.avg, top1.avg\n","\n","def validate_multilabel(val_loader, model, classifier, criterion, opt):\n","    \"\"\"validation\"\"\"\n","    model.eval()\n","    classifier.eval()\n","    \n","    device = opt.device\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    label_list = []\n","    out_list = []\n","    \n","    with torch.no_grad():\n","        end = time.time()\n","#         for idx, (image, bio_tensor,eye_id,bcva,cst,patient) in enumerate(val_loader):\n","        for idx, (image, bio_tensor) in enumerate(val_loader):\n","            images = image.float().to(device)\n","\n","            labels = bio_tensor\n","            labels = labels.float()\n","            label_list.append(labels.squeeze().detach().cpu().numpy())\n","            labels = labels.to(device)\n","            bsz = labels.shape[0]\n","\n","            # forward\n","            output = classifier(model.encoder(images))\n","\n","            loss = criterion(output, labels)\n","            output = torch.round(torch.sigmoid(output))\n","            out_list.append(output.detach().cpu().numpy())\n","            \n","            # update metric\n","            losses.update(loss.item(), bsz)\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","\n","    label_array = np.array(label_list)\n","    out_array = np.array(out_list)\n","    out_array = np.concatenate(out_list, axis=0)\n","    print(\"output\",out_array,\"labels\",label_array)\n","    r = f1_score(label_array, out_array, average='macro')\n","\n","    return losses.avg, r\n","\n","def main_multilabel():\n","    best_acc = 0\n","    opt = parse_option(args)\n","\n","    # build data loader\n","    device = opt.device\n","    train_loader,  test_loader = set_loader_new(opt)\n","    val_loader = val_loader_fun(opt)\n","\n","    \n","    # training routine\n","    for i in range(0,1):\n","        r_list = []\n","        model, classifier, criterion = set_model(opt)\n","        optimizer = set_optimizer(opt, classifier)\n","        \n","        for epoch in range(1, opt.epochs + 1):\n","            adjust_learning_rate(opt, optimizer, epoch)\n","\n","            # train for one epoch\n","            time1 = time.time()\n","            loss, acc = train_OCT_multilabel(train_loader, model, classifier, criterion,\n","                                             optimizer, epoch, opt)\n","            time2 = time.time()\n","            \n","                \n","\n","    # eval for one epoch\n","            loss, r = validate_multilabel(test_loader, model, classifier, criterion, opt)\n","            print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}, f1_score:{:.2f}'.format(epoch, time2 - time1, acc, r))\n","\n","            r_list.append(r)\n","        \n","        print(r_list)\n","    \n","    df = pd.DataFrame({'AUROC': r_list})\n","    excel_name = opt.backbone_training + '_' + opt.biomarker + opt.model + str(opt.percentage) + 'multiAUROC' + str(opt.patient_split) + '.csv'\n","    df.to_csv(excel_name, index=False)\n","    \n","    submission_generate(val_loader, model,classifier, opt)"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Model**"]},{"cell_type":"code","execution_count":238,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.714916Z","iopub.status.busy":"2023-09-02T19:46:04.712108Z","iopub.status.idle":"2023-09-02T19:46:04.779772Z","shell.execute_reply":"2023-09-02T19:46:04.778895Z","shell.execute_reply.started":"2023-09-02T19:46:04.714884Z"},"trusted":true},"outputs":[],"source":["### from models.resnet import  SupConResNet,LinearClassifier,LinearClassifier_MultiLabel, SupConResNet_Original, SupConResNet_Original_Headless ###\n","### Resnet Model ###\n","\n","# ---------------------------------------------------------------------------------------------------#\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(BasicBlock, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out\n","\n","\n","# ---------------------------------------------------------------------------------------------------#\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1, is_last=False):\n","        super(Bottleneck, self).__init__()\n","        self.is_last = is_last\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        preact = out\n","        out = F.relu(out)\n","        if self.is_last:\n","            return out, preact\n","        else:\n","            return out\n","\n","\n","# ---------------------------------------------------------------------------------------------------#\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, in_channel=1, zero_init_residual=False):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        #self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1,bias=False)\n","        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves\n","        # like an identity. This improves the model by 0.2~0.3% according to:\n","        # https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for i in range(num_blocks):\n","            stride = strides[i]\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, layer=100):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","\n","        out = self.layer2(out)\n","\n","        out = self.layer3(out)\n","\n","        out = self.layer4(out)\n","\n","        out = self.avgpool(out)\n","        out = torch.flatten(out, 1)\n","        return out\n","\n","\n","# ---------------------------------------------------------------------------------------------------#\n","def resnet18(**kwargs):\n","    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n","\n","\n","def resnet34(**kwargs):\n","    return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n","\n","\n","def resnet50(**kwargs):\n","    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n","\n","\n","def resnet101(**kwargs):\n","    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n","\n","\n","model_dict = {\n","    'resnet18': [resnet18, 512],\n","    'resnet34': [resnet34, 512],\n","    'resnet50': [resnet50, 2048],\n","    'resnet101': [resnet101, 2048],\n","}\n","\n","\n","# ---------------------------------------------------------------------------------------------------#\n","class SupConResNet_Original(nn.Module):\n","    def __init__(self, name='resnet50',head='mlp',feat_dim=128,use_head=True):\n","        super(SupConResNet_Original,self).__init__()\n","        self.use_head = use_head\n","        if(name == 'resnet50'):\n","            self.encoder = torchvision.models.resnet50(zero_init_residual=True)\n","            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","            self.encoder.fc = nn.Identity()\n","\n","            if head == 'linear':\n","                self.head = nn.Linear(2048, feat_dim)\n","            elif head == 'mlp':\n","                self.head = nn.Sequential(\n","                    nn.Linear(2048, 2048),\n","                    nn.ReLU(inplace=True),\n","                    nn.Linear(2048, feat_dim)\n","                )\n","            else:\n","                raise NotImplementedError(\n","                    'head not supported: {}'.format(head))\n","\n","        else:\n","            self.encoder = torchvision.models.resnet18(zero_init_residual=True)\n","            self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","            self.encoder.fc = nn.Identity()\n","\n","            if head == 'linear':\n","                self.head = nn.Linear(512, feat_dim)\n","            elif head == 'mlp':\n","                self.head = nn.Sequential(\n","                    nn.Linear(512, 512),\n","                    nn.ReLU(inplace=True),\n","                    nn.Linear(512, feat_dim)\n","                )\n","            else:\n","                raise NotImplementedError(\n","                    'head not supported: {}'.format(head))\n","\n","    def forward(self, x):\n","        feat = self.encoder(x)\n","\n","        feat = F.normalize(self.head(feat), dim=1)\n","        return feat\n","    \n","# # ---------------------------------------------------------------------------------------------------#\n","class LinearClassifier(nn.Module):\n","    \"\"\"Linear classifier\"\"\"\n","    def __init__(self, name='resnet50', num_classes=2):\n","        super(LinearClassifier, self).__init__()\n","        _, feat_dim = model_dict[name]\n","        self.fc = nn.Linear(feat_dim, num_classes)\n","        self.sigm = nn.Sigmoid()\n","\n","    def forward(self, features):\n","        return self.sigm(self.fc(features))\n","\n","# ---------------------------------------------------------------------------------------------------#\n","# class LinearClassifier(nn.Module):\n","#     \"\"\"Linear classifier\"\"\"\n","#     def __init__(self, name='resnet50', num_classes=2):\n","#         super(LinearClassifier, self).__init__()\n","#         _, feat_dim = model_dict[name]\n","        \n","#         hidden_dim = 512\n","        \n","#         self.fc = nn.Sequential(\n","#             nn.Linear(feat_dim, hidden_dim),   # Additional layer 1\n","#             nn.ReLU(),\n","#             nn.Linear(hidden_dim, num_classes) # Output layer\n","#         )\n","#         self.sigm = nn.Sigmoid()\n","\n","#     def forward(self, features):\n","#         return self.sigm(self.fc(features))\n","\n","\n","# # ---------------------------------------------------------------------------------------------------#\n","# class LinearClassifier_MultiLabel(nn.Module):\n","#     \"\"\"Linear classifier\"\"\"\n","#     def __init__(self, name='resnet50', num_classes=2):\n","#         super(LinearClassifier_MultiLabel, self).__init__()\n","#         _, feat_dim = model_dict[name]\n","# #         self.fc = nn.Linear(feat_dim, feat_dim)\n","# #         self.sigm = nn.Sigmoid()\n","#         self.fc = nn.Linear(feat_dim, num_classes)\n","#         self.sigm = nn.Sigmoid()\n","\n","#     def forward(self, features):\n","#         return self.sigm(self.fc(features))"]},{"cell_type":"markdown","metadata":{},"source":["* #   **Data loaders**"]},{"cell_type":"code","execution_count":239,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.787005Z","iopub.status.busy":"2023-09-02T19:46:04.784879Z","iopub.status.idle":"2023-09-02T19:46:04.837399Z","shell.execute_reply":"2023-09-02T19:46:04.836518Z","shell.execute_reply.started":"2023-09-02T19:46:04.786971Z"},"papermill":{"duration":0.336349,"end_time":"2023-08-24T16:44:11.560134","exception":false,"start_time":"2023-08-24T16:44:11.223785","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["### from utils.utils import AverageMeter,warmup_learning_rate ###\n","### setting the model and the loaders ###\n","\n","# from datasets.biomarker_multi_fusion import BiomarkerDatasetAttributes_MultiLabel_MultiClass\n","\n","# ---------------------------------------------------------------------------------------------------#\n","def set_model(opt):\n","        \n","    if(opt.multi == 1 and opt.super!=3):\n","        model = SupConResNet_Original(name=opt.model)\n","        criterion = torch.nn.BCEWithLogitsLoss()\n","        classifier = LinearClassifier(name=opt.model, num_classes=6)\n","        \n","    elif(opt.multi == 0):\n","        model = SupConResNet_Original(name=opt.model)\n","        criterion = torch.nn.BCEWithLogitsLoss()\n","#         criterion = torch.nn.BCELoss()\n","#         criterion = torch.nn.CrossEntropyLoss()\n","        classifier = LinearClassifier(name=opt.model, num_classes=1)\n","        \n","    ckpt = torch.load(opt.ckpt, map_location='cpu')\n","    state_dict = ckpt['model']\n","    device = opt.device\n","    if torch.cuda.is_available():\n","        if opt.parallel == 0:\n","            model.encoder = torch.nn.DataParallel(model.encoder)\n","        else:\n","            new_state_dict = {}\n","            for k, v in state_dict.items():\n","                k = k.replace(\"module.\", \"\")\n","                new_state_dict[k] = v\n","            state_dict = new_state_dict\n","        model = model.to(device)\n","        classifier = classifier.to(device)\n","        criterion = criterion.to(device)\n","        cudnn.benchmark = True\n","\n","        model.load_state_dict(state_dict)\n","\n","    return model, classifier, criterion\n","# ---------------------------------------------------------------------------------------------------#\n","def set_loader_new(opt):\n","    \n","    # construct data loader\n","    if opt.dataset == 'Prime':\n","        mean = (.1706)\n","        std = (.2112)\n","    else:\n","        raise ValueError('dataset not supported: {}'.format(opt.dataset))\n","\n","# ---------------------------------------------------------------------------------------------------#\n","    normalize = transforms.Normalize(mean=mean, std=std)\n","\n","    train_transform = transforms.Compose([\n","        transforms.RandomResizedCrop(size=224, scale=(0.2, 1.)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n","        transforms.RandomGrayscale(p=0.2),\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","\n","    val_transform = transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","    \n","# ---------------------------------------------------------------------------------------------------#        \n","    if opt.dataset =='Prime':\n","        data_path_train = opt.train_image_path\n","        csv_path_train = opt.train_csv_path\n","        csv_path_test = opt.test_csv_path\n","        data_path_test = opt.test_image_path\n","          \n","        if(opt.multi == 1 and opt.super !=3):\n","            train_dataset = BiomarkerDatasetAttributes_MultiLabel(csv_path_train, data_path_train, transforms=train_transform)\n","            test_dataset = BiomarkerDatasetAttributes_MultiLabel(csv_path_test, data_path_test, transforms=val_transform)\n","        else:\n","            train_dataset = BiomarkerDatasetAttributes(csv_path_train,data_path_train,transforms = train_transform)\n","            test_dataset = BiomarkerDatasetAttributes_Validate(csv_path_test,data_path_test,transforms = val_transform)\n","    else:\n","        raise ValueError(opt.dataset)\n","# ---------------------------------------------------------------------------------------------------#        \n","    train_loader = torch.utils.data.DataLoader(\n","                    train_dataset, batch_size=opt.batch_size, shuffle=True,\n","                    num_workers=opt.num_workers, pin_memory=True)\n","# ---------------------------------------------------------------------------------------------------# \n","    if(opt.biomarker == 'drt' and opt.patient_split == 1):\n","        dl = True\n","    elif(opt.multi == 1):\n","        dl = True\n","    else:\n","        dl=False\n","# ---------------------------------------------------------------------------------------------------#        \n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=1, shuffle=True,\n","        num_workers=0, pin_memory=True,drop_last=dl)\n","\n","    return train_loader, test_loader\n","\n","# ---------------------------------------------------------------------------------------------------#        \n","class TwoCropTransform:\n","    \"\"\"Create two crops of the same image\"\"\"\n","    def __init__(self, transform):\n","        self.transform = transform\n","\n","    def __call__(self, x):\n","        return [self.transform(x), self.transform(x)]\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","        \n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        _, gt = target.topk(maxk, 1, True, True)\n","        gt = gt.t()\n","        correct = pred.eq(gt.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res\n","    \n","def accuracy_single(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        _, pred = output.topk(maxk, 1, True, True)\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res\n","\n","\n","def adjust_learning_rate(args, optimizer, epoch):\n","    lr = args.learning_rate\n","    \n","    if args.cosine:\n","        eta_min = lr * (args.lr_decay_rate ** 3)\n","        lr = eta_min + (lr - eta_min) * (\n","                1 + math.cos(math.pi * epoch / args.epochs)) / 2\n","    else:\n","        steps = np.sum(epoch > np.asarray(args.lr_decay_epochs))\n","        if steps > 0:\n","            lr = lr * (args.lr_decay_rate ** steps)\n","            \n","    print(lr)\n","    \n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","\n","def warmup_learning_rate(args, epoch, batch_id, total_batches, optimizer):\n","    \n","    if args.warm and epoch <= args.warm_epochs:\n","        p = (batch_id + (epoch - 1) * total_batches) / \\\n","            (args.warm_epochs * total_batches)\n","        lr = args.warmup_from + p * (args.warmup_to - args.warmup_from)\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","\n","def set_optimizer(opt, model):\n","\n","    optimizer = optim.SGD(model.parameters(),\n","                          lr=opt.learning_rate,\n","                          momentum=opt.momentum,\n","                          weight_decay=opt.weight_decay)\n","\n","\n","    return optimizer\n","\n","\n","def save_model(model, optimizer, opt, epoch, save_file):\n","    \n","    print('==> Saving...')\n","    state = {\n","        'opt': opt,\n","        'model': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'epoch': epoch,\n","    }\n","    torch.save(state, save_file)\n","    del state\n","\n","def accuracy_multilabel(output,target):\n","    output = output.detach().cpu().numpy()\n","    target = target.detach().cpu().numpy()\n","    r = roc_auc_score(target,output,multi_class='ovr')\n","    print(r)"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Data Readers**"]},{"cell_type":"code","execution_count":240,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.843865Z","iopub.status.busy":"2023-09-02T19:46:04.841741Z","iopub.status.idle":"2023-09-02T19:46:04.857282Z","shell.execute_reply":"2023-09-02T19:46:04.856386Z","shell.execute_reply.started":"2023-09-02T19:46:04.843831Z"},"trusted":true},"outputs":[],"source":["### from datasets.biomarker_multi import BiomarkerDatasetAttributes_MultiLabel ###\n","\n","# elif(opt.multi == 1 and opt.super !=3): #\n","\n","\n","class BiomarkerDatasetAttributes_MultiLabel(data.Dataset):\n","    def __init__(self,df, img_dir, transforms):\n","        self.img_dir = img_dir\n","        self.transforms = transforms\n","        self.df = pd.read_csv(df)\n","#         self.clinical_dir = pd.read_csv(clinical_dir)\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        temp_path = self.df.iloc[idx,0][1:9]\n","        if temp_path == \"TREX DME\":\n","            path = self.img_dir + \"/TREX_DME\" + self.df.iloc[idx,0]\n","        else:\n","            path = self.img_dir  + self.df.iloc[idx,0]\n","        image = Image.open(path).convert(\"L\")\n","        image = np.array(image)\n","        image = Image.fromarray(image)\n","        image = self.transforms(image)\n","        b1 = self.df.iloc[idx,1]\n","        b2 = self.df.iloc[idx,2]\n","        b3 = self.df.iloc[idx,3]\n","        b4 = self.df.iloc[idx, 4]\n","        b5 = self.df.iloc[idx, 5]\n","        b6 = self.df.iloc[idx, 6]\n","        bio_tensor = torch.tensor([b1, b2, b3, b4, b5, b6])\n","        #assert self.df.iloc[idx,0] == self.clinical_dir.iloc[idx,0]\n","        \n","#         c1 = (self.df.iloc[idx,7])\n","#         c2 = (self.df.iloc[idx,8])\n","        \n","#         eye_id = (self.df.iloc[idx,9])\n","#         patient_id = (self.df.iloc[idx,10])\n","        \n","#         return image, bio_tensor, eye_id, c1, c2, patient_id\n","        return image, bio_tensor"]},{"cell_type":"code","execution_count":241,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.864751Z","iopub.status.busy":"2023-09-02T19:46:04.861851Z","iopub.status.idle":"2023-09-02T19:46:04.881469Z","shell.execute_reply":"2023-09-02T19:46:04.880587Z","shell.execute_reply.started":"2023-09-02T19:46:04.864718Z"},"trusted":true},"outputs":[],"source":["class BiomarkerDatasetAttributes(data.Dataset):\n","    def __init__(self,df, img_dir, transforms):\n","        self.img_dir = img_dir\n","        self.transforms = transforms\n","        self.df = pd.read_csv(df)\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        temp_path = self.df.iloc[idx,0][1:9]\n","        if temp_path == \"TREX DME\":\n","            path = self.img_dir + \"/TREX_DME\" + self.df.iloc[idx,0]\n","        else:\n","            path = self.img_dir  + self.df.iloc[idx,0]\n","        image = Image.open(path).convert(\"L\")\n","        image = np.array(image)\n","        image = Image.fromarray(image)\n","        image = self.transforms(image)\n","        \n","        ir_hrf = torch.tensor([self.df.iloc[idx,1]])\n","        partial_vit = torch.tensor([self.df.iloc[idx,2]])\n","        full_vit = torch.tensor([self.df.iloc[idx,3]])\n","        vit_deb = torch.tensor([self.df.iloc[idx,4]])\n","        drt = torch.tensor([self.df.iloc[idx,5]])\n","        fluid_irf = torch.tensor([self.df.iloc[idx,6]])\n","        bcva = torch.tensor([self.df.iloc[idx,7]])\n","        cst = torch.tensor([self.df.iloc[idx,8]])\n","        eye_id = torch.tensor([self.df.iloc[idx,9]])\n","        patient = torch.tensor([self.df.iloc[idx,10]])\n","        \n","#         ir_hrf = self.df.iloc[idx,1]\n","#         partial_vit = self.df.iloc[idx,2]\n","#         full_vit = self.df.iloc[idx,3]\n","#         vit_deb = self.df.iloc[idx,4]\n","#         drt = self.df.iloc[idx,5]\n","#         fluid_irf = self.df.iloc[idx,6]\n","#         bcva = self.df.iloc[idx,7]\n","#         cst = self.df.iloc[idx,8]\n","#         eye_id = self.df.iloc[idx,9]\n","#         patient = self.df.iloc[idx,10]\n","\n","        \n","        \n","        return image, vit_deb,ir_hrf, full_vit,partial_vit,fluid_irf,drt,eye_id,bcva,cst,patient"]},{"cell_type":"code","execution_count":242,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.884980Z","iopub.status.busy":"2023-09-02T19:46:04.884298Z","iopub.status.idle":"2023-09-02T19:46:04.898037Z","shell.execute_reply":"2023-09-02T19:46:04.897269Z","shell.execute_reply.started":"2023-09-02T19:46:04.884950Z"},"trusted":true},"outputs":[],"source":["class BiomarkerDatasetAttributes_Validate(data.Dataset):\n","    def __init__(self,df, img_dir, transforms):\n","        self.img_dir = img_dir\n","        self.transforms = transforms\n","        self.df = pd.read_csv(df)\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        temp_path = self.df.iloc[idx,0][1:9]\n","        if temp_path == \"TREX DME\":\n","            path = self.img_dir + \"/TREX_DME\" + self.df.iloc[idx,0]\n","        else:\n","            path = self.img_dir  + self.df.iloc[idx,0]\n","        image = Image.open(path).convert(\"L\")\n","        image = np.array(image)\n","        image = Image.fromarray(image)\n","        image = self.transforms(image)\n","        \n","        \n","        ir_hrf = torch.tensor([self.df.iloc[idx,1]])\n","        partial_vit = torch.tensor([self.df.iloc[idx,2]])\n","        full_vit = torch.tensor([self.df.iloc[idx,3]])\n","        vit_deb = torch.tensor([self.df.iloc[idx,4]])\n","        drt = torch.tensor([self.df.iloc[idx,5]])\n","        fluid_irf = torch.tensor([self.df.iloc[idx,6]])\n","        \n","#         ir_hrf = self.df.iloc[idx,1]\n","#         partial_vit = self.df.iloc[idx,2]\n","#         full_vit = self.df.iloc[idx,3]\n","#         vit_deb = self.df.iloc[idx,4]\n","#         drt = self.df.iloc[idx,5]\n","#         fluid_irf = self.df.iloc[idx,6]\n","\n","        return image, vit_deb,ir_hrf, full_vit,partial_vit,fluid_irf,drt"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Arguments Specifying**"]},{"cell_type":"code","execution_count":243,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.902174Z","iopub.status.busy":"2023-09-02T19:46:04.901563Z","iopub.status.idle":"2023-09-02T19:46:04.915120Z","shell.execute_reply":"2023-09-02T19:46:04.914357Z","shell.execute_reply.started":"2023-09-02T19:46:04.902129Z"},"papermill":{"duration":0.016286,"end_time":"2023-08-24T16:44:11.902828","exception":false,"start_time":"2023-08-24T16:44:11.886542","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["args = ('--batch_size', '128', '--patient_split', '1', '--model', 'resnet50', \n","        '--biomarker' ,'ir_hrf', '--backbone_training', 'BCVA', '--dataset' ,'Prime', '--epochs', '25',\n","        '--device', 'cuda:0' ,'--super', '0', '--multi', '0',\n","        '--train_csv_path', '/kaggle/input/modified-training-biomarkers-and-clinical/Modified training biomarkers and clinical.csv' ,\n","        '--test_csv_path', '/kaggle/input/leaked-answers/Leaked answers.csv' ,\n","        '--ckpt', '/kaggle/input/contrastive-output/last.pth',\n","        '--train_image_path' ,'/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TRAIN/OLIVES',\n","        '--test_image_path', '/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/',\n","        '--val_csv_path','/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv',\n","        '--val_image_path','/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/')"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Submission Data reading**"]},{"cell_type":"code","execution_count":244,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.917300Z","iopub.status.busy":"2023-09-02T19:46:04.916604Z","iopub.status.idle":"2023-09-02T19:46:04.933268Z","shell.execute_reply":"2023-09-02T19:46:04.932416Z","shell.execute_reply.started":"2023-09-02T19:46:04.917269Z"},"trusted":true},"outputs":[],"source":["class RECOVERY_TEST(data.Dataset):\n","    def __init__(self,df, img_dir, transforms):\n","        self.img_dir = img_dir\n","        self.transforms = transforms\n","        self.df = pd.read_csv(df)\n","       \n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        temp_path = self.df.iloc[idx,0][0:9]\n","        if temp_path == \"/TREX DME\":\n","            path = self.img_dir + \"/TREX_DME\" + self.df.iloc[idx,0]\n","        else:\n","            path = self.img_dir  + self.df.iloc[idx,0]\n","        image = Image.open(path).convert(\"L\")\n","        image = np.array(image)\n","        image = Image.fromarray(image)\n","        image = self.transforms(image)\n","       \n","        return image , self.df.iloc[idx,0]"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Submission data loading**"]},{"cell_type":"code","execution_count":245,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.938796Z","iopub.status.busy":"2023-09-02T19:46:04.937493Z","iopub.status.idle":"2023-09-02T19:46:04.949270Z","shell.execute_reply":"2023-09-02T19:46:04.948394Z","shell.execute_reply.started":"2023-09-02T19:46:04.938765Z"},"trusted":true},"outputs":[],"source":["def val_loader_fun(opt):\n","    mean = (.1706)\n","    std = (.2112)\n","    normalize = transforms.Normalize(mean=mean, std=std)\n","    val_transform = transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","    csv_path_val = opt.val_csv_path\n","    data_path_val = opt.val_image_path\n","    val_dataset = RECOVERY_TEST(csv_path_val,data_path_val ,transforms = val_transform)\n","    val_loader = torch.utils.data.DataLoader(\n","        val_dataset, batch_size=1, shuffle=False,\n","        num_workers=0, pin_memory=True,drop_last=False)\n","\n","    return val_loader"]},{"cell_type":"markdown","metadata":{},"source":["# *   **Submission generating**"]},{"cell_type":"code","execution_count":246,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.955795Z","iopub.status.busy":"2023-09-02T19:46:04.953478Z","iopub.status.idle":"2023-09-02T19:46:04.971279Z","shell.execute_reply":"2023-09-02T19:46:04.970397Z","shell.execute_reply.started":"2023-09-02T19:46:04.955763Z"},"trusted":true},"outputs":[],"source":["def submission_generate(val_loader, model, classifier, opt):\n","    \"\"\"validation\"\"\"\n","    model.eval()\n","    classifier.eval()\n","    device = opt.device\n","    out_list = []\n","    with torch.no_grad():\n","        for idx, (image, temp_path) in enumerate(val_loader):\n","            images = image.float().to(device)\n","            output = classifier(model.encoder(images))\n","            output = torch.round(torch.sigmoid(output))\n","            out_list.append((output.squeeze().detach().cpu().numpy() , temp_path))\n","    \n","    sub_dir_csv = '/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv'\n","    sub_dir = pd.read_csv(sub_dir_csv)\n","    print(len(sub_dir))\n","    print(sub_dir,out_list)\n","    for i in range(0,len(sub_dir)):\n","        #print(sub_dir.iloc[i,0] , out_list[i][1][0])\n","        assert sub_dir.iloc[i,0] == out_list[i][1][0]\n","        \n","        sub_dir.iloc[i,1] = out_list[i][0][0]\n","        sub_dir.iloc[i, 2] = out_list[i][0][1]\n","        sub_dir.iloc[i, 3] = out_list[i][0][2]\n","        sub_dir.iloc[i, 4] = out_list[i][0][3]\n","        sub_dir.iloc[i, 5] = out_list[i][0][4]\n","        sub_dir.iloc[i, 6] = out_list[i][0][5]\n","        \n","    print(sub_dir.head())\n","    sub_dir.to_csv( f'/kaggle/working/baseline_result_{datetime.datetime.now()}.csv',index=False)"]},{"cell_type":"code","execution_count":247,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.978362Z","iopub.status.busy":"2023-09-02T19:46:04.975881Z","iopub.status.idle":"2023-09-02T19:46:04.990310Z","shell.execute_reply":"2023-09-02T19:46:04.989405Z","shell.execute_reply.started":"2023-09-02T19:46:04.978331Z"},"trusted":true},"outputs":[],"source":["def submission_generate_single(val_loader, model, classifier, opt):\n","    \"\"\"validation\"\"\"\n","    model.eval()\n","    classifier.eval()\n","    device = opt.device\n","    out_list = []\n","    with torch.no_grad():\n","        for idx, (image, temp_path) in enumerate(val_loader):\n","            images = image.float().to(device)\n","            output = classifier(model.encoder(images))\n","            output = torch.round(torch.sigmoid(output))\n","            out_list.append((output.squeeze().detach().cpu().numpy() , temp_path))\n","    \n","    sub_dir_csv = '/kaggle/input/olives-vip-cup-2023/olives/2023 IEEE SPS Video and Image Processing (VIP) Cup - Ophthalmic Biomarker Detection/TEST/test_set_submission_template.csv'\n","    sub_dir = pd.read_csv(sub_dir_csv)\n","    print(len(sub_dir))\n","#     print(sub_dir,out_list)\n","    for i in range(0,len(sub_dir)):\n","        #print(sub_dir.iloc[i,0] , out_list[i][1][0])\n","        assert sub_dir.iloc[i,0] == out_list[i][1][0]\n","        \n","        sub_dir.iloc[i,1] = out_list[i][0]\n","        \n","    print(sub_dir.head())\n","    sub_dir.to_csv( f'/kaggle/working/baseline_result_{datetime.datetime.now()}.csv',index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# *   **The first function called**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-02T19:46:04.997610Z","iopub.status.busy":"2023-09-02T19:46:04.994776Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.005\n","Train: [1][10/73]\tBT 0.174 (0.510)\tDT 0.000 (0.334)\tloss 0.653 (0.638)\tAcc@1 35.938 (32.969)\n","Train: [1][20/73]\tBT 0.174 (0.488)\tDT 0.000 (0.312)\tloss 0.620 (0.632)\tAcc@1 29.688 (31.836)\n","Train: [1][30/73]\tBT 0.174 (0.488)\tDT 0.000 (0.312)\tloss 0.620 (0.632)\tAcc@1 29.688 (31.875)\n","Train: [1][40/73]\tBT 0.174 (0.492)\tDT 0.000 (0.316)\tloss 0.652 (0.631)\tAcc@1 35.938 (31.758)\n","Train: [1][50/73]\tBT 0.174 (0.489)\tDT 0.000 (0.313)\tloss 0.612 (0.631)\tAcc@1 28.906 (31.859)\n","Train: [1][60/73]\tBT 0.174 (0.485)\tDT 0.000 (0.309)\tloss 0.638 (0.631)\tAcc@1 33.594 (31.992)\n","Train: [1][70/73]\tBT 0.174 (0.493)\tDT 0.000 (0.317)\tloss 0.603 (0.632)\tAcc@1 27.344 (32.199)\n","zeros False ones True\n","Train epoch 1, total time 35.74, accuracy:32.24, f1_score:0.2372\n","0.005\n","Train: [2][10/73]\tBT 0.184 (0.519)\tDT 0.008 (0.342)\tloss 0.625 (0.635)\tAcc@1 31.250 (33.203)\n","Train: [2][20/73]\tBT 0.174 (0.502)\tDT 0.000 (0.327)\tloss 0.614 (0.633)\tAcc@1 29.688 (32.852)\n","Train: [2][30/73]\tBT 0.174 (0.490)\tDT 0.000 (0.314)\tloss 0.595 (0.633)\tAcc@1 26.562 (32.891)\n","Train: [2][40/73]\tBT 0.174 (0.501)\tDT 0.000 (0.326)\tloss 0.627 (0.628)\tAcc@1 32.031 (32.148)\n","Train: [2][50/73]\tBT 0.174 (0.493)\tDT 0.000 (0.318)\tloss 0.618 (0.630)\tAcc@1 30.469 (32.453)\n","Train: [2][60/73]\tBT 0.200 (0.488)\tDT 0.026 (0.313)\tloss 0.600 (0.630)\tAcc@1 27.344 (32.435)\n","Train: [2][70/73]\tBT 0.174 (0.486)\tDT 0.000 (0.311)\tloss 0.599 (0.629)\tAcc@1 27.344 (32.433)\n","zeros False ones True\n","Train epoch 2, total time 35.32, accuracy:32.24, f1_score:0.2372\n","0.005\n","Train: [3][10/73]\tBT 0.174 (0.577)\tDT 0.000 (0.402)\tloss 0.642 (0.626)\tAcc@1 35.156 (32.188)\n","Train: [3][20/73]\tBT 0.194 (0.523)\tDT 0.016 (0.347)\tloss 0.684 (0.625)\tAcc@1 42.188 (32.148)\n","Train: [3][30/73]\tBT 0.175 (0.507)\tDT 0.001 (0.331)\tloss 0.615 (0.628)\tAcc@1 30.469 (32.630)\n","Train: [3][40/73]\tBT 0.174 (0.496)\tDT 0.000 (0.321)\tloss 0.618 (0.624)\tAcc@1 31.250 (31.992)\n"]},{"name":"stderr","output_type":"stream","text":["Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ce9c903b9a0>\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():Exception ignored in: \n","<function _MultiProcessingDataLoaderIter.__del__ at 0x7ce9c903b9a0>  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","\n","Traceback (most recent call last):\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","\n","AssertionError    self._shutdown_workers(): \n","can only test a child process\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionError: can only test a child process\n"]},{"name":"stdout","output_type":"stream","text":["Train: [3][50/73]\tBT 0.198 (0.506)\tDT 0.024 (0.330)\tloss 0.610 (0.625)\tAcc@1 29.688 (32.328)\n","Train: [3][60/73]\tBT 0.181 (0.499)\tDT 0.003 (0.323)\tloss 0.644 (0.623)\tAcc@1 35.938 (32.005)\n","Train: [3][70/73]\tBT 0.197 (0.495)\tDT 0.024 (0.319)\tloss 0.606 (0.624)\tAcc@1 28.906 (32.154)\n","zeros False ones True\n","Train epoch 3, total time 36.23, accuracy:32.24, f1_score:0.2372\n","0.005\n"]},{"name":"stderr","output_type":"stream","text":["Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ce9c903b9a0>\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    self._shutdown_workers()\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    Exception ignored in: if w.is_alive():\n","  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'\n","<function _MultiProcessingDataLoaderIter.__del__ at 0x7ce9c903b9a0>AssertionError: \n","Traceback (most recent call last):\n","can only test a child process  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n","    \n","self._shutdown_workers()\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n","    if w.is_alive():\n","  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n","    assert self._parent_pid == os.getpid(), 'can only test a child process'\n","AssertionError: can only test a child process\n"]},{"name":"stdout","output_type":"stream","text":["Train: [4][10/73]\tBT 0.174 (0.599)\tDT 0.000 (0.423)\tloss 0.609 (0.618)\tAcc@1 29.688 (31.328)\n","Train: [4][20/73]\tBT 0.178 (0.534)\tDT 0.000 (0.359)\tloss 0.634 (0.624)\tAcc@1 34.375 (32.344)\n","Train: [4][30/73]\tBT 0.174 (0.517)\tDT 0.000 (0.342)\tloss 0.665 (0.625)\tAcc@1 39.062 (32.656)\n","Train: [4][40/73]\tBT 0.174 (0.505)\tDT 0.000 (0.330)\tloss 0.638 (0.626)\tAcc@1 35.156 (32.734)\n","Train: [4][50/73]\tBT 0.174 (0.512)\tDT 0.000 (0.337)\tloss 0.584 (0.623)\tAcc@1 26.562 (32.281)\n","Train: [4][60/73]\tBT 0.174 (0.508)\tDT 0.000 (0.333)\tloss 0.582 (0.621)\tAcc@1 25.781 (32.083)\n","Train: [4][70/73]\tBT 0.173 (0.504)\tDT 0.000 (0.329)\tloss 0.566 (0.621)\tAcc@1 23.438 (32.042)\n","zeros False ones True\n","Train epoch 4, total time 36.48, accuracy:32.24, f1_score:0.2372\n","0.005\n","Train: [5][10/73]\tBT 0.180 (0.515)\tDT 0.006 (0.335)\tloss 0.636 (0.623)\tAcc@1 35.156 (32.734)\n","Train: [5][20/73]\tBT 0.316 (0.531)\tDT 0.142 (0.353)\tloss 0.590 (0.613)\tAcc@1 27.344 (31.172)\n","Train: [5][30/73]\tBT 0.293 (0.511)\tDT 0.118 (0.335)\tloss 0.649 (0.616)\tAcc@1 36.719 (31.562)\n","Train: [5][40/73]\tBT 0.309 (0.502)\tDT 0.135 (0.326)\tloss 0.624 (0.616)\tAcc@1 32.812 (31.621)\n","Train: [5][50/73]\tBT 0.385 (0.496)\tDT 0.211 (0.321)\tloss 0.635 (0.619)\tAcc@1 34.375 (32.062)\n","Train: [5][60/73]\tBT 0.393 (0.493)\tDT 0.219 (0.317)\tloss 0.641 (0.620)\tAcc@1 35.938 (32.266)\n","Train: [5][70/73]\tBT 0.358 (0.489)\tDT 0.185 (0.314)\tloss 0.607 (0.620)\tAcc@1 30.469 (32.232)\n","zeros False ones True\n","Train epoch 5, total time 35.58, accuracy:32.24, f1_score:0.2372\n","0.005\n","Train: [6][10/73]\tBT 0.191 (0.508)\tDT 0.014 (0.330)\tloss 0.610 (0.618)\tAcc@1 31.250 (32.266)\n","Train: [6][20/73]\tBT 0.183 (0.491)\tDT 0.009 (0.312)\tloss 0.607 (0.617)\tAcc@1 30.469 (32.148)\n","Train: [6][30/73]\tBT 0.198 (0.482)\tDT 0.024 (0.305)\tloss 0.609 (0.615)\tAcc@1 30.469 (31.797)\n","Train: [6][40/73]\tBT 0.187 (0.478)\tDT 0.010 (0.300)\tloss 0.628 (0.614)\tAcc@1 33.594 (31.738)\n","Train: [6][50/73]\tBT 0.207 (0.475)\tDT 0.033 (0.297)\tloss 0.618 (0.616)\tAcc@1 32.031 (32.062)\n","Train: [6][60/73]\tBT 0.269 (0.485)\tDT 0.095 (0.308)\tloss 0.597 (0.618)\tAcc@1 28.906 (32.253)\n","Train: [6][70/73]\tBT 0.263 (0.484)\tDT 0.089 (0.307)\tloss 0.644 (0.617)\tAcc@1 36.719 (32.199)\n","zeros False ones False\n","Train epoch 6, total time 35.13, accuracy:32.24, f1_score:0.2393\n","0.005\n","Train: [7][10/73]\tBT 0.200 (0.520)\tDT 0.026 (0.345)\tloss 0.611 (0.603)\tAcc@1 31.250 (30.156)\n","Train: [7][20/73]\tBT 0.174 (0.494)\tDT 0.000 (0.318)\tloss 0.638 (0.612)\tAcc@1 35.156 (31.406)\n","Train: [7][30/73]\tBT 0.174 (0.508)\tDT 0.001 (0.332)\tloss 0.618 (0.615)\tAcc@1 32.812 (31.953)\n","Train: [7][40/73]\tBT 0.195 (0.499)\tDT 0.022 (0.323)\tloss 0.610 (0.617)\tAcc@1 31.250 (32.363)\n","Train: [7][50/73]\tBT 0.183 (0.494)\tDT 0.009 (0.318)\tloss 0.637 (0.616)\tAcc@1 35.938 (32.234)\n","Train: [7][60/73]\tBT 0.185 (0.489)\tDT 0.011 (0.313)\tloss 0.647 (0.617)\tAcc@1 37.500 (32.448)\n","Train: [7][70/73]\tBT 0.296 (0.489)\tDT 0.122 (0.312)\tloss 0.609 (0.616)\tAcc@1 30.469 (32.199)\n","zeros False ones False\n","Train epoch 7, total time 35.52, accuracy:32.24, f1_score:0.2535\n","0.005\n","Train: [8][10/73]\tBT 0.174 (0.512)\tDT 0.000 (0.336)\tloss 0.603 (0.623)\tAcc@1 30.469 (33.516)\n","Train: [8][20/73]\tBT 0.174 (0.494)\tDT 0.000 (0.318)\tloss 0.619 (0.619)\tAcc@1 32.812 (32.891)\n","Train: [8][30/73]\tBT 0.174 (0.483)\tDT 0.000 (0.307)\tloss 0.593 (0.614)\tAcc@1 28.125 (32.057)\n","Train: [8][40/73]\tBT 0.174 (0.480)\tDT 0.000 (0.304)\tloss 0.644 (0.612)\tAcc@1 36.719 (31.797)\n","Train: [8][50/73]\tBT 0.174 (0.478)\tDT 0.000 (0.303)\tloss 0.659 (0.613)\tAcc@1 38.281 (31.953)\n","Train: [8][60/73]\tBT 0.174 (0.476)\tDT 0.000 (0.301)\tloss 0.582 (0.612)\tAcc@1 27.344 (31.940)\n","Train: [8][70/73]\tBT 0.174 (0.484)\tDT 0.000 (0.308)\tloss 0.634 (0.614)\tAcc@1 35.938 (32.154)\n","zeros False ones False\n","Train epoch 8, total time 35.16, accuracy:32.24, f1_score:0.2787\n","0.005\n","Train: [9][10/73]\tBT 0.174 (0.534)\tDT 0.000 (0.358)\tloss 0.619 (0.627)\tAcc@1 32.812 (34.375)\n","Train: [9][20/73]\tBT 0.209 (0.502)\tDT 0.036 (0.326)\tloss 0.629 (0.621)\tAcc@1 34.375 (33.320)\n","Train: [9][30/73]\tBT 0.211 (0.488)\tDT 0.036 (0.311)\tloss 0.632 (0.617)\tAcc@1 35.156 (32.839)\n","Train: [9][40/73]\tBT 0.184 (0.501)\tDT 0.010 (0.325)\tloss 0.614 (0.616)\tAcc@1 32.812 (32.695)\n","Train: [9][50/73]\tBT 0.254 (0.494)\tDT 0.081 (0.318)\tloss 0.623 (0.617)\tAcc@1 33.594 (32.828)\n","Train: [9][60/73]\tBT 0.308 (0.491)\tDT 0.132 (0.315)\tloss 0.626 (0.613)\tAcc@1 35.156 (32.201)\n","Train: [9][70/73]\tBT 0.177 (0.496)\tDT 0.000 (0.320)\tloss 0.609 (0.613)\tAcc@1 32.031 (32.232)\n","zeros False ones False\n","Train epoch 9, total time 35.87, accuracy:32.24, f1_score:0.3157\n","0.005\n","Train: [10][10/73]\tBT 0.174 (0.593)\tDT 0.000 (0.417)\tloss 0.615 (0.619)\tAcc@1 32.812 (33.281)\n","Train: [10][20/73]\tBT 0.174 (0.528)\tDT 0.000 (0.353)\tloss 0.628 (0.613)\tAcc@1 35.156 (32.461)\n","Train: [10][30/73]\tBT 0.174 (0.529)\tDT 0.000 (0.354)\tloss 0.610 (0.611)\tAcc@1 32.031 (32.057)\n","Train: [10][40/73]\tBT 0.174 (0.514)\tDT 0.000 (0.339)\tloss 0.640 (0.614)\tAcc@1 35.938 (32.520)\n","Train: [10][50/73]\tBT 0.174 (0.506)\tDT 0.001 (0.331)\tloss 0.618 (0.613)\tAcc@1 32.812 (32.469)\n","Train: [10][60/73]\tBT 0.174 (0.501)\tDT 0.000 (0.325)\tloss 0.623 (0.611)\tAcc@1 33.594 (32.148)\n","Train: [10][70/73]\tBT 0.174 (0.498)\tDT 0.000 (0.323)\tloss 0.555 (0.611)\tAcc@1 23.438 (32.154)\n","zeros False ones False\n","Train epoch 10, total time 36.37, accuracy:32.24, f1_score:0.3538\n","0.005\n","Train: [11][10/73]\tBT 0.174 (0.535)\tDT 0.000 (0.359)\tloss 0.596 (0.616)\tAcc@1 29.688 (32.969)\n","Train: [11][20/73]\tBT 0.188 (0.504)\tDT 0.009 (0.328)\tloss 0.605 (0.610)\tAcc@1 32.031 (32.227)\n","Train: [11][30/73]\tBT 0.336 (0.496)\tDT 0.161 (0.320)\tloss 0.632 (0.612)\tAcc@1 35.938 (32.552)\n","Train: [11][40/73]\tBT 0.270 (0.490)\tDT 0.092 (0.314)\tloss 0.568 (0.614)\tAcc@1 25.000 (32.812)\n","Train: [11][50/73]\tBT 0.257 (0.500)\tDT 0.083 (0.324)\tloss 0.561 (0.611)\tAcc@1 23.438 (32.453)\n","Train: [11][60/73]\tBT 0.251 (0.497)\tDT 0.078 (0.322)\tloss 0.594 (0.610)\tAcc@1 29.688 (32.266)\n","Train: [11][70/73]\tBT 0.336 (0.495)\tDT 0.153 (0.319)\tloss 0.624 (0.610)\tAcc@1 35.156 (32.277)\n","zeros False ones False\n","Train epoch 11, total time 35.98, accuracy:32.24, f1_score:0.3920\n","0.005\n","Train: [12][10/73]\tBT 0.174 (0.538)\tDT 0.000 (0.363)\tloss 0.569 (0.603)\tAcc@1 26.562 (31.328)\n","Train: [12][20/73]\tBT 0.174 (0.548)\tDT 0.000 (0.373)\tloss 0.631 (0.608)\tAcc@1 35.938 (32.148)\n","Train: [12][30/73]\tBT 0.174 (0.527)\tDT 0.000 (0.351)\tloss 0.635 (0.609)\tAcc@1 35.938 (32.109)\n","Train: [12][40/73]\tBT 0.203 (0.513)\tDT 0.025 (0.336)\tloss 0.641 (0.610)\tAcc@1 37.500 (32.402)\n","Train: [12][50/73]\tBT 0.174 (0.504)\tDT 0.000 (0.329)\tloss 0.583 (0.609)\tAcc@1 28.125 (32.281)\n","Train: [12][60/73]\tBT 0.174 (0.501)\tDT 0.000 (0.326)\tloss 0.620 (0.611)\tAcc@1 34.375 (32.526)\n","Train: [12][70/73]\tBT 0.174 (0.499)\tDT 0.000 (0.323)\tloss 0.628 (0.609)\tAcc@1 35.938 (32.333)\n","zeros False ones False\n","Train epoch 12, total time 36.20, accuracy:32.24, f1_score:0.4228\n","0.005\n","Train: [13][10/73]\tBT 0.174 (0.512)\tDT 0.000 (0.337)\tloss 0.621 (0.602)\tAcc@1 35.156 (31.562)\n","Train: [13][20/73]\tBT 0.178 (0.492)\tDT 0.000 (0.317)\tloss 0.632 (0.606)\tAcc@1 36.719 (32.070)\n","Train: [13][30/73]\tBT 0.178 (0.488)\tDT 0.000 (0.312)\tloss 0.582 (0.602)\tAcc@1 28.125 (31.484)\n","Train: [13][40/73]\tBT 0.180 (0.483)\tDT 0.006 (0.308)\tloss 0.629 (0.604)\tAcc@1 35.938 (31.758)\n","Train: [13][50/73]\tBT 0.176 (0.487)\tDT 0.000 (0.311)\tloss 0.559 (0.604)\tAcc@1 24.219 (31.812)\n","Train: [13][60/73]\tBT 0.174 (0.491)\tDT 0.000 (0.316)\tloss 0.563 (0.605)\tAcc@1 25.000 (32.018)\n","Train: [13][70/73]\tBT 0.174 (0.489)\tDT 0.000 (0.314)\tloss 0.594 (0.606)\tAcc@1 28.906 (32.054)\n","zeros False ones False\n","Train epoch 13, total time 35.48, accuracy:32.24, f1_score:0.4587\n","0.005\n","Train: [14][10/73]\tBT 0.174 (0.523)\tDT 0.000 (0.347)\tloss 0.645 (0.610)\tAcc@1 39.062 (32.969)\n","Train: [14][20/73]\tBT 0.174 (0.498)\tDT 0.000 (0.322)\tloss 0.618 (0.606)\tAcc@1 34.375 (32.344)\n","Train: [14][30/73]\tBT 0.174 (0.512)\tDT 0.000 (0.337)\tloss 0.592 (0.607)\tAcc@1 30.469 (32.552)\n","Train: [14][40/73]\tBT 0.175 (0.501)\tDT 0.001 (0.327)\tloss 0.603 (0.608)\tAcc@1 32.812 (32.715)\n","Train: [14][50/73]\tBT 0.177 (0.497)\tDT 0.003 (0.322)\tloss 0.587 (0.606)\tAcc@1 29.688 (32.359)\n","Train: [14][60/73]\tBT 0.175 (0.494)\tDT 0.001 (0.319)\tloss 0.562 (0.605)\tAcc@1 25.000 (32.253)\n","Train: [14][70/73]\tBT 0.184 (0.494)\tDT 0.000 (0.319)\tloss 0.598 (0.606)\tAcc@1 31.250 (32.388)\n","zeros False ones False\n","Train epoch 14, total time 35.80, accuracy:32.24, f1_score:0.4927\n","0.005\n","Train: [15][10/73]\tBT 0.174 (0.510)\tDT 0.000 (0.335)\tloss 0.590 (0.606)\tAcc@1 29.688 (32.578)\n","Train: [15][20/73]\tBT 0.189 (0.487)\tDT 0.010 (0.310)\tloss 0.622 (0.606)\tAcc@1 33.594 (32.539)\n","Train: [15][30/73]\tBT 0.192 (0.481)\tDT 0.019 (0.304)\tloss 0.617 (0.604)\tAcc@1 34.375 (32.188)\n","Train: [15][40/73]\tBT 0.224 (0.483)\tDT 0.050 (0.306)\tloss 0.633 (0.606)\tAcc@1 36.719 (32.480)\n","Train: [15][50/73]\tBT 0.183 (0.480)\tDT 0.010 (0.303)\tloss 0.613 (0.606)\tAcc@1 33.594 (32.375)\n","Train: [15][60/73]\tBT 0.199 (0.485)\tDT 0.024 (0.308)\tloss 0.664 (0.606)\tAcc@1 42.969 (32.396)\n","Train: [15][70/73]\tBT 0.174 (0.485)\tDT 0.000 (0.308)\tloss 0.615 (0.605)\tAcc@1 33.594 (32.310)\n","zeros False ones False\n","Train epoch 15, total time 35.22, accuracy:32.24, f1_score:0.5167\n","0.005\n","Train: [16][10/73]\tBT 0.174 (0.525)\tDT 0.000 (0.350)\tloss 0.607 (0.610)\tAcc@1 32.812 (33.438)\n","Train: [16][20/73]\tBT 0.174 (0.495)\tDT 0.000 (0.320)\tloss 0.594 (0.603)\tAcc@1 30.469 (32.305)\n","Train: [16][30/73]\tBT 0.174 (0.488)\tDT 0.000 (0.313)\tloss 0.586 (0.602)\tAcc@1 28.906 (31.927)\n","Train: [16][40/73]\tBT 0.180 (0.498)\tDT 0.002 (0.322)\tloss 0.581 (0.604)\tAcc@1 28.125 (32.344)\n","Train: [16][50/73]\tBT 0.174 (0.493)\tDT 0.000 (0.317)\tloss 0.570 (0.603)\tAcc@1 27.344 (32.219)\n","Train: [16][60/73]\tBT 0.174 (0.490)\tDT 0.000 (0.315)\tloss 0.650 (0.603)\tAcc@1 39.844 (32.135)\n","Train: [16][70/73]\tBT 0.174 (0.490)\tDT 0.000 (0.315)\tloss 0.630 (0.603)\tAcc@1 36.719 (32.232)\n","zeros False ones False\n","Train epoch 16, total time 35.56, accuracy:32.24, f1_score:0.5425\n","0.005\n","Train: [17][10/73]\tBT 0.174 (0.588)\tDT 0.000 (0.408)\tloss 0.625 (0.602)\tAcc@1 35.938 (32.031)\n","Train: [17][20/73]\tBT 0.182 (0.528)\tDT 0.001 (0.350)\tloss 0.587 (0.602)\tAcc@1 29.688 (32.188)\n","Train: [17][30/73]\tBT 0.174 (0.511)\tDT 0.000 (0.334)\tloss 0.584 (0.598)\tAcc@1 28.906 (31.484)\n","Train: [17][40/73]\tBT 0.174 (0.499)\tDT 0.000 (0.322)\tloss 0.613 (0.600)\tAcc@1 35.156 (31.875)\n","Train: [17][50/73]\tBT 0.174 (0.493)\tDT 0.000 (0.317)\tloss 0.654 (0.602)\tAcc@1 40.625 (32.219)\n","Train: [17][60/73]\tBT 0.174 (0.488)\tDT 0.000 (0.312)\tloss 0.611 (0.603)\tAcc@1 32.812 (32.383)\n","Train: [17][70/73]\tBT 0.185 (0.501)\tDT 0.000 (0.324)\tloss 0.605 (0.603)\tAcc@1 32.031 (32.254)\n","zeros False ones False\n","Train epoch 17, total time 36.71, accuracy:32.24, f1_score:0.5622\n","0.005\n","Train: [18][10/73]\tBT 0.174 (0.515)\tDT 0.000 (0.340)\tloss 0.582 (0.602)\tAcc@1 29.688 (32.266)\n","Train: [18][20/73]\tBT 0.174 (0.498)\tDT 0.000 (0.324)\tloss 0.641 (0.599)\tAcc@1 38.281 (32.031)\n","Train: [18][30/73]\tBT 0.174 (0.487)\tDT 0.000 (0.313)\tloss 0.624 (0.601)\tAcc@1 35.938 (32.188)\n","Train: [18][40/73]\tBT 0.323 (0.516)\tDT 0.141 (0.340)\tloss 0.625 (0.601)\tAcc@1 36.719 (32.129)\n","Train: [18][50/73]\tBT 0.217 (0.510)\tDT 0.043 (0.335)\tloss 0.637 (0.603)\tAcc@1 38.281 (32.516)\n","Train: [18][60/73]\tBT 0.228 (0.508)\tDT 0.054 (0.333)\tloss 0.626 (0.602)\tAcc@1 35.938 (32.344)\n","Train: [18][70/73]\tBT 0.276 (0.504)\tDT 0.103 (0.329)\tloss 0.557 (0.602)\tAcc@1 25.000 (32.433)\n","zeros False ones False\n","Train epoch 18, total time 36.67, accuracy:32.24, f1_score:0.5803\n","0.005\n","Train: [19][10/73]\tBT 0.185 (0.515)\tDT 0.002 (0.338)\tloss 0.619 (0.605)\tAcc@1 34.375 (32.734)\n","Train: [19][20/73]\tBT 0.174 (0.524)\tDT 0.000 (0.348)\tloss 0.610 (0.599)\tAcc@1 32.031 (31.641)\n","Train: [19][30/73]\tBT 0.176 (0.512)\tDT 0.001 (0.337)\tloss 0.601 (0.599)\tAcc@1 34.375 (31.979)\n","Train: [19][40/73]\tBT 0.174 (0.500)\tDT 0.000 (0.325)\tloss 0.556 (0.596)\tAcc@1 24.219 (31.641)\n","Train: [19][50/73]\tBT 0.187 (0.492)\tDT 0.013 (0.317)\tloss 0.603 (0.597)\tAcc@1 32.031 (31.750)\n","Train: [19][60/73]\tBT 0.181 (0.489)\tDT 0.008 (0.313)\tloss 0.635 (0.598)\tAcc@1 39.844 (31.875)\n","Train: [19][70/73]\tBT 0.180 (0.484)\tDT 0.007 (0.308)\tloss 0.642 (0.599)\tAcc@1 39.844 (32.121)\n","zeros False ones False\n","Train epoch 19, total time 35.24, accuracy:32.24, f1_score:0.5848\n","0.005\n","Train: [20][10/73]\tBT 0.174 (0.518)\tDT 0.000 (0.344)\tloss 0.583 (0.595)\tAcc@1 31.250 (31.797)\n","Train: [20][20/73]\tBT 0.177 (0.492)\tDT 0.003 (0.318)\tloss 0.614 (0.601)\tAcc@1 33.594 (32.734)\n","Train: [20][30/73]\tBT 0.174 (0.486)\tDT 0.000 (0.312)\tloss 0.549 (0.602)\tAcc@1 24.219 (32.786)\n","Train: [20][40/73]\tBT 0.174 (0.482)\tDT 0.000 (0.307)\tloss 0.590 (0.599)\tAcc@1 29.688 (32.324)\n","Train: [20][50/73]\tBT 0.186 (0.490)\tDT 0.002 (0.316)\tloss 0.598 (0.600)\tAcc@1 31.250 (32.344)\n","Train: [20][60/73]\tBT 0.174 (0.489)\tDT 0.000 (0.314)\tloss 0.582 (0.597)\tAcc@1 28.906 (31.797)\n","Train: [20][70/73]\tBT 0.174 (0.487)\tDT 0.000 (0.313)\tloss 0.641 (0.599)\tAcc@1 42.188 (32.243)\n","zeros False ones False\n","Train epoch 20, total time 35.37, accuracy:32.24, f1_score:0.5895\n","0.005\n","Train: [21][10/73]\tBT 0.174 (0.526)\tDT 0.000 (0.350)\tloss 0.648 (0.608)\tAcc@1 39.844 (33.594)\n","Train: [21][20/73]\tBT 0.191 (0.496)\tDT 0.015 (0.320)\tloss 0.646 (0.605)\tAcc@1 40.625 (33.242)\n","Train: [21][30/73]\tBT 0.174 (0.511)\tDT 0.000 (0.335)\tloss 0.607 (0.599)\tAcc@1 32.812 (32.266)\n","Train: [21][40/73]\tBT 0.183 (0.500)\tDT 0.009 (0.324)\tloss 0.595 (0.597)\tAcc@1 32.812 (31.914)\n","Train: [21][50/73]\tBT 0.192 (0.491)\tDT 0.018 (0.315)\tloss 0.608 (0.595)\tAcc@1 35.938 (31.656)\n","Train: [21][60/73]\tBT 0.295 (0.489)\tDT 0.121 (0.313)\tloss 0.595 (0.597)\tAcc@1 30.469 (31.953)\n","Train: [21][70/73]\tBT 0.329 (0.488)\tDT 0.153 (0.312)\tloss 0.592 (0.598)\tAcc@1 32.812 (32.210)\n","zeros False ones False\n","Train epoch 21, total time 35.41, accuracy:32.24, f1_score:0.5980\n","0.005\n","Train: [22][10/73]\tBT 0.180 (0.524)\tDT 0.006 (0.348)\tloss 0.618 (0.603)\tAcc@1 35.156 (33.047)\n","Train: [22][20/73]\tBT 0.174 (0.502)\tDT 0.000 (0.326)\tloss 0.602 (0.603)\tAcc@1 32.812 (33.125)\n","Train: [22][30/73]\tBT 0.194 (0.489)\tDT 0.021 (0.313)\tloss 0.606 (0.601)\tAcc@1 33.594 (33.021)\n","Train: [22][40/73]\tBT 0.219 (0.486)\tDT 0.045 (0.310)\tloss 0.600 (0.603)\tAcc@1 32.031 (33.320)\n","Train: [22][50/73]\tBT 0.257 (0.486)\tDT 0.083 (0.311)\tloss 0.596 (0.601)\tAcc@1 32.031 (32.953)\n","Train: [22][60/73]\tBT 0.261 (0.497)\tDT 0.087 (0.321)\tloss 0.595 (0.599)\tAcc@1 30.469 (32.552)\n","Train: [22][70/73]\tBT 0.315 (0.494)\tDT 0.141 (0.318)\tloss 0.572 (0.598)\tAcc@1 26.562 (32.344)\n","zeros False ones False\n","Train epoch 22, total time 35.81, accuracy:32.24, f1_score:0.6006\n","0.005\n","Train: [23][10/73]\tBT 0.178 (0.528)\tDT 0.000 (0.349)\tloss 0.641 (0.600)\tAcc@1 39.844 (32.656)\n","Train: [23][20/73]\tBT 0.195 (0.501)\tDT 0.022 (0.323)\tloss 0.602 (0.594)\tAcc@1 32.812 (31.797)\n","Train: [23][30/73]\tBT 0.199 (0.495)\tDT 0.022 (0.317)\tloss 0.620 (0.595)\tAcc@1 35.938 (31.875)\n","Train: [23][40/73]\tBT 0.179 (0.504)\tDT 0.005 (0.326)\tloss 0.587 (0.593)\tAcc@1 30.469 (31.699)\n","Train: [23][50/73]\tBT 0.176 (0.496)\tDT 0.002 (0.318)\tloss 0.587 (0.597)\tAcc@1 29.688 (32.172)\n","Train: [23][60/73]\tBT 0.174 (0.491)\tDT 0.000 (0.313)\tloss 0.582 (0.597)\tAcc@1 30.469 (32.227)\n","Train: [23][70/73]\tBT 0.180 (0.487)\tDT 0.003 (0.310)\tloss 0.571 (0.595)\tAcc@1 29.688 (31.964)\n","zeros False ones False\n","Train epoch 23, total time 35.37, accuracy:32.24, f1_score:0.6018\n","0.005\n","Train: [24][10/73]\tBT 0.178 (0.592)\tDT 0.000 (0.414)\tloss 0.613 (0.599)\tAcc@1 35.938 (32.891)\n","Train: [24][20/73]\tBT 0.179 (0.533)\tDT 0.000 (0.356)\tloss 0.585 (0.589)\tAcc@1 29.688 (31.211)\n","Train: [24][30/73]\tBT 0.174 (0.509)\tDT 0.000 (0.333)\tloss 0.613 (0.588)\tAcc@1 36.719 (31.146)\n","Train: [24][40/73]\tBT 0.174 (0.500)\tDT 0.000 (0.324)\tloss 0.670 (0.592)\tAcc@1 46.094 (31.738)\n","Train: [24][50/73]\tBT 0.198 (0.492)\tDT 0.024 (0.317)\tloss 0.601 (0.593)\tAcc@1 32.812 (31.859)\n","Train: [24][60/73]\tBT 0.189 (0.490)\tDT 0.016 (0.314)\tloss 0.556 (0.595)\tAcc@1 24.219 (32.096)\n","Train: [24][70/73]\tBT 0.180 (0.494)\tDT 0.000 (0.318)\tloss 0.591 (0.596)\tAcc@1 30.469 (32.266)\n","zeros False ones False\n","Train epoch 24, total time 36.02, accuracy:32.24, f1_score:0.6055\n","0.005\n","Train: [25][10/73]\tBT 0.174 (0.514)\tDT 0.000 (0.339)\tloss 0.581 (0.603)\tAcc@1 28.906 (33.281)\n","Train: [25][20/73]\tBT 0.174 (0.492)\tDT 0.000 (0.316)\tloss 0.594 (0.594)\tAcc@1 32.031 (31.680)\n","Train: [25][30/73]\tBT 0.174 (0.485)\tDT 0.000 (0.310)\tloss 0.658 (0.594)\tAcc@1 42.188 (31.875)\n","Train: [25][40/73]\tBT 0.174 (0.482)\tDT 0.000 (0.307)\tloss 0.590 (0.593)\tAcc@1 30.469 (31.934)\n","Train: [25][50/73]\tBT 0.174 (0.492)\tDT 0.000 (0.317)\tloss 0.612 (0.594)\tAcc@1 35.938 (32.062)\n","Train: [25][60/73]\tBT 0.174 (0.488)\tDT 0.000 (0.313)\tloss 0.606 (0.595)\tAcc@1 35.156 (32.292)\n","Train: [25][70/73]\tBT 0.174 (0.487)\tDT 0.000 (0.312)\tloss 0.582 (0.594)\tAcc@1 32.031 (32.121)\n"]}],"source":["### main linear function ###\n","\n","\n","# from training_linear.training_one_epoch_ckpt_multi import main_multilabel\n","\n","\n","try:\n","    import apex\n","    from apex import amp, optimizers\n","except ImportError:\n","    pass\n","\n","\n","if __name__ == '__main__':\n","    opt = parse_option(args)\n","    # opt.super --> Supervised (1) or Not (0) or (2) Fusion Supervised or (3) BCE Loss for AUROC\n","    # opt.multi --> MultiLabel (1) or Not(0)\n","    # 0 --> Ckpt Training\n","    # multi 1 and super 3 --> BCE Individual Biomarkers\n","\n","    if(opt.multi == 1 and (opt.super == 0 or opt.super ==8)):\n","        main_multilabel()\n","\n","    else:\n","        main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
